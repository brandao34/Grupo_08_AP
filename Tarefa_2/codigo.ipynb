{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Pré-processamento com Pandas/Scikit-learn\n",
    "# ==============================================\n",
    "\n",
    "# Carregar e combinar datasets\n",
    "#inputs_df = pd.read_csv('Dataset6_clean_input.csv')\n",
    "#outputs_df = pd.read_csv('Dataset6_clean_output.csv')\n",
    "#df = pd.merge(inputs_df, outputs_df, on='ID')\n",
    "df = pd.read_csv('../Tarefa_1/Dataset6_clean.csv' , sep=',')\n",
    "# Vetorização do texto\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Text']).toarray()\n",
    "y = df['Label'].map({'AI':1, 'Human':0}).values\n",
    "\n",
    "# Divisão treino-teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Modelos Implementados com Numpy\n",
    "# ==============================================\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(linear)\n",
    "            \n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self._sigmoid(linear)\n",
    "        return (y_pred > 0.5).astype(int)\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.01, dropout_rate=0.0):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.lr = lr\n",
    "        self.dropout_rate = dropout_rate  # Fraction of neurons to drop (0.0 means no dropout)\n",
    "        self.loss_history = []  # To monitor loss over epochs\n",
    "\n",
    "    def _relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def _relu_deriv(self, Z):\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    def _sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        # First layer forward\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self._relu(self.z1)\n",
    "        \n",
    "        # Apply dropout during training\n",
    "        if training and self.dropout_rate > 0.0:\n",
    "            # Create dropout mask: 1 means keep, 0 means drop.\n",
    "            self.dropout_mask = (np.random.rand(*self.a1.shape) > self.dropout_rate).astype(float)\n",
    "            # Apply inverted dropout scaling so that no scaling is needed at test time.\n",
    "            self.a1 *= self.dropout_mask\n",
    "            self.a1 /= (1.0 - self.dropout_rate)\n",
    "        else:\n",
    "            # If not training, no dropout mask is used.\n",
    "            self.dropout_mask = None\n",
    "        \n",
    "        # Second layer forward\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.output = self._sigmoid(self.z2)\n",
    "        return self.output\n",
    "\n",
    "    def compute_loss(self, y, output):\n",
    "        # Binary cross-entropy loss with a small epsilon for numerical stability\n",
    "        epsilon = 1e-15\n",
    "        y = y.reshape(-1, 1)\n",
    "        loss = -np.mean(y * np.log(output + epsilon) + (1 - y) * np.log(1 - output + epsilon))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        m = y.shape[0]\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Compute gradients for the output layer\n",
    "        dz2 = output - y\n",
    "        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Backpropagate to the hidden layer\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self._relu_deriv(self.z1)\n",
    "        # If dropout was applied, propagate the mask\n",
    "        if self.dropout_rate > 0.0 and self.dropout_mask is not None:\n",
    "            dz1 *= self.dropout_mask\n",
    "        \n",
    "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def train(self, X, y, epochs=1000, print_loss=True):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass with dropout enabled\n",
    "            output = self.forward(X, training=True)\n",
    "            \n",
    "            # Compute and store loss\n",
    "            loss = self.compute_loss(y, output)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Optionally print loss and accuracy at intervals\n",
    "            if print_loss and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                predictions = self.predict(X)\n",
    "                accuracy = np.mean(predictions.flatten() == y.flatten())\n",
    "                print(f\"Epoch {epoch}: Loss = {loss}, Accuracy = {accuracy*100:.2f}%\")\n",
    "            \n",
    "            # Backward pass and weight update\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y, output)\n",
    "            self.W1 -= self.lr * dW1\n",
    "            self.b1 -= self.lr * db1\n",
    "            self.W2 -= self.lr * dW2\n",
    "            self.b2 -= self.lr * db2\n",
    "\n",
    "    def predict(self, X):\n",
    "        # During prediction, disable dropout by setting training=False\n",
    "        output = self.forward(X, training=False)\n",
    "        return (output > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.01, dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the RNN with given parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: Number of features per time step.\n",
    "        - hidden_size: Number of hidden units.\n",
    "        - output_size: Number of output units (for binary classification, this is 1).\n",
    "        - lr: Learning rate.\n",
    "        - dropout_rate: Fraction of hidden units to drop during training (0.0 means no dropout).\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = lr\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Weight initialization:\n",
    "        # Input to hidden weights\n",
    "        self.W_xh = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        # Hidden to hidden weights (recurrent connections)\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        # Bias for hidden layer\n",
    "        self.b_h = np.zeros((1, hidden_size))\n",
    "        # Hidden to output weights\n",
    "        self.W_hy = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        # Bias for output layer\n",
    "        self.b_y = np.zeros((1, output_size))\n",
    "        \n",
    "        self.loss_history = []  # To store loss values over epochs\n",
    "\n",
    "    def _tanh(self, x):\n",
    "        \"\"\"Tanh activation function.\"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _tanh_deriv(self, x):\n",
    "        \"\"\"Derivative of tanh activation function: 1 - tanh(x)^2.\"\"\"\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input array with shape (batch_size, seq_length, input_size)\n",
    "        - training: Boolean flag indicating whether we are training (enables dropout)\n",
    "        \n",
    "        Returns:\n",
    "        - output: Final predictions with shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = X.shape\n",
    "        # Initialize arrays to store hidden states and pre-activation values\n",
    "        self.h = np.zeros((batch_size, seq_length, self.hidden_size))\n",
    "        self.z = np.zeros((batch_size, seq_length, self.hidden_size))\n",
    "        \n",
    "        # Initialize previous hidden state as zeros\n",
    "        h_prev = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        # Process each time step\n",
    "        for t in range(seq_length):\n",
    "            # Linear combination for hidden state at time t\n",
    "            self.z[:, t, :] = np.dot(X[:, t, :], self.W_xh) + np.dot(h_prev, self.W_hh) + self.b_h\n",
    "            h_current = self._tanh(self.z[:, t, :])\n",
    "            \n",
    "            # Apply dropout to the hidden state if in training mode\n",
    "            if training and self.dropout_rate > 0.0:\n",
    "                # Create dropout mask: 1 indicates keep neuron, 0 indicates drop\n",
    "                dropout_mask = (np.random.rand(*h_current.shape) > self.dropout_rate).astype(float)\n",
    "                h_current *= dropout_mask\n",
    "                # Inverted dropout scaling to maintain the expected value at test time\n",
    "                h_current /= (1.0 - self.dropout_rate)\n",
    "            \n",
    "            self.h[:, t, :] = h_current\n",
    "            h_prev = h_current\n",
    "        \n",
    "        # Use the last hidden state to compute the output\n",
    "        self.output_linear = np.dot(h_prev, self.W_hy) + self.b_y\n",
    "        self.output = self._sigmoid(self.output_linear)\n",
    "        return self.output\n",
    "\n",
    "    def compute_loss(self, y, output):\n",
    "        \"\"\"\n",
    "        Computes binary cross-entropy loss.\n",
    "        \n",
    "        Parameters:\n",
    "        - y: True labels with shape (batch_size,)\n",
    "        - output: Predicted outputs with shape (batch_size, 1)\n",
    "        \n",
    "        Returns:\n",
    "        - loss: Average binary cross-entropy loss over the batch\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15  # Small constant for numerical stability\n",
    "        y = y.reshape(-1, 1)\n",
    "        loss = -np.mean(y * np.log(output + epsilon) + (1 - y) * np.log(1 - output + epsilon))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        \"\"\"\n",
    "        Performs backpropagation through time (BPTT) to compute gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input array with shape (batch_size, seq_length, input_size)\n",
    "        - y: True labels with shape (batch_size,)\n",
    "        - output: Predicted outputs from the forward pass\n",
    "        \n",
    "        Returns:\n",
    "        - dW_xh: Gradient for input-to-hidden weights\n",
    "        - dW_hh: Gradient for hidden-to-hidden weights\n",
    "        - db_h: Gradient for hidden biases\n",
    "        - dW_hy: Gradient for hidden-to-output weights\n",
    "        - db_y: Gradient for output biases\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = X.shape\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Gradients for output layer\n",
    "        d_output = output - y  # (batch_size, 1)\n",
    "        dW_hy = np.dot(self.h[:, -1, :].T, d_output) / batch_size\n",
    "        db_y = np.sum(d_output, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Initialize gradients for recurrent weights and biases\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        \n",
    "        # Gradient from the output layer to the last hidden state\n",
    "        dh = np.dot(d_output, self.W_hy.T)  # (batch_size, hidden_size)\n",
    "        dh_next = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        # Backpropagation through time (iterate backwards over time steps)\n",
    "        for t in reversed(range(seq_length)):\n",
    "            # Total gradient for the hidden state at time t\n",
    "            dh_total = dh + dh_next\n",
    "            dz = dh_total * self._tanh_deriv(self.z[:, t, :])\n",
    "            \n",
    "            # Gradients for the input-to-hidden weights\n",
    "            dW_xh += np.dot(X[:, t, :].T, dz) / batch_size\n",
    "            \n",
    "            # Determine the previous hidden state (zero if t == 0)\n",
    "            h_prev = self.h[:, t-1, :] if t > 0 else np.zeros((batch_size, self.hidden_size))\n",
    "            dW_hh += np.dot(h_prev.T, dz) / batch_size\n",
    "            db_h += np.sum(dz, axis=0, keepdims=True) / batch_size\n",
    "            \n",
    "            # Propagate gradient to previous time step\n",
    "            dh_next = np.dot(dz, self.W_hh.T)\n",
    "        \n",
    "        return dW_xh, dW_hh, db_h, dW_hy, db_y\n",
    "\n",
    "    def train(self, X, y, epochs=1000, print_loss=True):\n",
    "        \"\"\"\n",
    "        Trains the RNN over a specified number of epochs.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input array with shape (batch_size, seq_length, input_size)\n",
    "        - y: Labels with shape (batch_size,)\n",
    "        - epochs: Number of training iterations\n",
    "        - print_loss: Flag to print loss and accuracy at intervals\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass (with dropout enabled during training)\n",
    "            output = self.forward(X, training=True)\n",
    "            \n",
    "            # Compute and store loss\n",
    "            loss = self.compute_loss(y, output)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Optionally print loss and accuracy every 100 epochs (or on the final epoch)\n",
    "            if print_loss and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                predictions = self.predict(X)\n",
    "                accuracy = np.mean(predictions.flatten() == y.flatten())\n",
    "                print(f\"Epoch {epoch}: Loss = {loss}, Accuracy = {accuracy*100:.2f}%\")\n",
    "            \n",
    "            # Backward pass and weight update\n",
    "            dW_xh, dW_hh, db_h, dW_hy, db_y = self.backward(X, y, output)\n",
    "            self.W_xh -= self.lr * dW_xh\n",
    "            self.W_hh -= self.lr * dW_hh\n",
    "            self.b_h -= self.lr * db_h\n",
    "            self.W_hy -= self.lr * dW_hy\n",
    "            self.b_y -= self.lr * db_y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the output for a given input.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input array with shape (batch_size, seq_length, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        - Binary predictions with shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Disable dropout during prediction\n",
    "        output = self.forward(X, training=False)\n",
    "        return (output > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       422\n",
      "           1       0.99      0.91      0.95       390\n",
      "\n",
      "    accuracy                           0.95       812\n",
      "   macro avg       0.96      0.95      0.95       812\n",
      "weighted avg       0.96      0.95      0.95       812\n",
      "\n",
      "[[420   2]\n",
      " [ 36 354]]\n",
      "LR Test Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Treinar Regressão Logística\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr = LogisticRegression(lr=0.08, n_iters=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_acc = np.mean(lr.predict(X_test) == y_test)\n",
    "lr_pred = lr.predict(X_test)\n",
    "print(classification_report(y_test,lr_pred))\n",
    "print(confusion_matrix(y_test,lr_pred))\n",
    "print(f\"LR Test Accuracy: {lr_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Neural Network...\n",
      "Epoch 0: Loss = 0.6931665233546033, Accuracy = 47.83%\n",
      "Epoch 100: Loss = 0.6924588774727781, Accuracy = 51.77%\n",
      "Epoch 200: Loss = 0.6923222058228619, Accuracy = 51.77%\n",
      "Epoch 300: Loss = 0.6920778759246174, Accuracy = 51.77%\n",
      "Epoch 400: Loss = 0.6916446413645758, Accuracy = 51.77%\n",
      "Epoch 500: Loss = 0.6907545403856307, Accuracy = 51.77%\n",
      "Epoch 600: Loss = 0.6890333150173179, Accuracy = 51.77%\n",
      "Epoch 700: Loss = 0.685667152598539, Accuracy = 51.77%\n",
      "Epoch 800: Loss = 0.6793924252709512, Accuracy = 53.16%\n",
      "Epoch 900: Loss = 0.6671298750840063, Accuracy = 70.48%\n",
      "Epoch 999: Loss = 0.6458058621854854, Accuracy = 88.94%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.90       422\n",
      "           1       1.00      0.77      0.87       390\n",
      "\n",
      "    accuracy                           0.89       812\n",
      "   macro avg       0.91      0.88      0.89       812\n",
      "weighted avg       0.91      0.89      0.89       812\n",
      "\n",
      "[[421   1]\n",
      " [ 89 301]]\n"
     ]
    }
   ],
   "source": [
    "# Treinar Rede Neural\n",
    "print(\"\\nTraining Neural Network...\")\n",
    "nn = NeuralNetwork(input_size=X_train.shape[1], hidden_size=64, \n",
    "                   output_size=1, lr=0.08, dropout_rate=0.2)\n",
    "nn.train(X_train, y_train, epochs=1000)\n",
    "nn_pred = nn.predict(X_test)\n",
    "print(classification_report(y_test,nn_pred))\n",
    "print(confusion_matrix(y_test,nn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RNN...\n",
      "Epoch 0: Loss = 0.6931534880106127, Accuracy = 48.47%\n",
      "Epoch 100: Loss = 0.6922954745947354, Accuracy = 51.77%\n",
      "Epoch 200: Loss = 0.6918956117584041, Accuracy = 51.77%\n",
      "Epoch 300: Loss = 0.6911021904864952, Accuracy = 51.77%\n",
      "Epoch 400: Loss = 0.6894707604885866, Accuracy = 51.77%\n",
      "Epoch 500: Loss = 0.6859929063216463, Accuracy = 51.77%\n",
      "Epoch 600: Loss = 0.6786207268005767, Accuracy = 53.19%\n",
      "Epoch 700: Loss = 0.6636614135041534, Accuracy = 72.36%\n",
      "Epoch 800: Loss = 0.6342197246335989, Accuracy = 90.26%\n",
      "Epoch 900: Loss = 0.5819973560504849, Accuracy = 95.16%\n",
      "Epoch 999: Loss = 0.5041302239109895, Accuracy = 96.73%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97       422\n",
      "           1       0.99      0.93      0.96       390\n",
      "\n",
      "    accuracy                           0.96       812\n",
      "   macro avg       0.97      0.96      0.96       812\n",
      "weighted avg       0.96      0.96      0.96       812\n",
      "\n",
      "[[419   3]\n",
      " [ 27 363]]\n"
     ]
    }
   ],
   "source": [
    "X_train_seq = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test_seq = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "print(\"\\nTraining RNN...\")\n",
    "# Ensure that X_train_seq has shape (batch_size, seq_length, input_size)\n",
    "rnn = RNN(input_size=X_train_seq.shape[2], hidden_size=64, \n",
    "          output_size=1, lr=0.08, dropout_rate=0.2)\n",
    "rnn.train(X_train_seq, y_train, epochs=1000)\n",
    "rnn_pred = rnn.predict(X_test_seq)\n",
    "print(classification_report(y_test, rnn_pred))\n",
    "print(confusion_matrix(y_test, rnn_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
