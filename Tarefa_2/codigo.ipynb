{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Pré-processamento com Pandas/Scikit-learn\n",
    "# ==============================================\n",
    "\n",
    "# Carregar e combinar datasets\n",
    "#inputs_df = pd.read_csv('Dataset6_clean_input.csv')\n",
    "#outputs_df = pd.read_csv('Dataset6_clean_output.csv')\n",
    "#df = pd.merge(inputs_df, outputs_df, on='ID')\n",
    "df = pd.read_csv('../Tarefa_1/Dataset6_clean.csv' , sep=',')\n",
    "# Vetorização do texto\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Text']).toarray()\n",
    "y = df['Label'].map({'AI':1, 'Human':0}).values\n",
    "\n",
    "# Divisão treino-teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Modelos Implementados com Numpy\n",
    "# ==============================================\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(linear)\n",
    "            \n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self._sigmoid(linear)\n",
    "        return (y_pred > 0.5).astype(int)\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.01, dropout_rate=0.0):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.lr = lr\n",
    "        self.dropout_rate = dropout_rate  # Fraction of neurons to drop (0.0 means no dropout)\n",
    "        self.loss_history = []  # To monitor loss over epochs\n",
    "\n",
    "    def _relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def _relu_deriv(self, Z):\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    def _sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        # First layer forward\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self._relu(self.z1)\n",
    "        \n",
    "        # Apply dropout during training\n",
    "        if training and self.dropout_rate > 0.0:\n",
    "            # Create dropout mask: 1 means keep, 0 means drop.\n",
    "            self.dropout_mask = (np.random.rand(*self.a1.shape) > self.dropout_rate).astype(float)\n",
    "            # Apply inverted dropout scaling so that no scaling is needed at test time.\n",
    "            self.a1 *= self.dropout_mask\n",
    "            self.a1 /= (1.0 - self.dropout_rate)\n",
    "        else:\n",
    "            # If not training, no dropout mask is used.\n",
    "            self.dropout_mask = None\n",
    "        \n",
    "        # Second layer forward\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.output = self._sigmoid(self.z2)\n",
    "        return self.output\n",
    "\n",
    "    def compute_loss(self, y, output):\n",
    "        # Binary cross-entropy loss with a small epsilon for numerical stability\n",
    "        epsilon = 1e-15\n",
    "        y = y.reshape(-1, 1)\n",
    "        loss = -np.mean(y * np.log(output + epsilon) + (1 - y) * np.log(1 - output + epsilon))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        m = y.shape[0]\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Compute gradients for the output layer\n",
    "        dz2 = output - y\n",
    "        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Backpropagate to the hidden layer\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self._relu_deriv(self.z1)\n",
    "        # If dropout was applied, propagate the mask\n",
    "        if self.dropout_rate > 0.0 and self.dropout_mask is not None:\n",
    "            dz1 *= self.dropout_mask\n",
    "        \n",
    "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def train(self, X, y, epochs=1000, print_loss=True):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass with dropout enabled\n",
    "            output = self.forward(X, training=True)\n",
    "            \n",
    "            # Compute and store loss\n",
    "            loss = self.compute_loss(y, output)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Optionally print loss and accuracy at intervals\n",
    "            if print_loss and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                predictions = self.predict(X)\n",
    "                accuracy = np.mean(predictions.flatten() == y.flatten())\n",
    "                print(f\"Epoch {epoch}: Loss = {loss}, Accuracy = {accuracy*100:.2f}%\")\n",
    "            \n",
    "            # Backward pass and weight update\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y, output)\n",
    "            self.W1 -= self.lr * dW1\n",
    "            self.b1 -= self.lr * db1\n",
    "            self.W2 -= self.lr * dW2\n",
    "            self.b2 -= self.lr * db2\n",
    "\n",
    "    def predict(self, X):\n",
    "        # During prediction, disable dropout by setting training=False\n",
    "        output = self.forward(X, training=False)\n",
    "        return (output > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       422\n",
      "           1       0.99      0.91      0.95       390\n",
      "\n",
      "    accuracy                           0.95       812\n",
      "   macro avg       0.96      0.95      0.95       812\n",
      "weighted avg       0.96      0.95      0.95       812\n",
      "\n",
      "[[420   2]\n",
      " [ 36 354]]\n",
      "LR Test Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Treinar Regressão Logística\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr = LogisticRegression(lr=0.08, n_iters=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_acc = np.mean(lr.predict(X_test) == y_test)\n",
    "lr_pred = lr.predict(X_test)\n",
    "print(classification_report(y_test,lr_pred))\n",
    "print(confusion_matrix(y_test,lr_pred))\n",
    "print(f\"LR Test Accuracy: {lr_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Neural Network...\n",
      "Epoch 0: Loss = 0.6931757891583803, Accuracy = 45.02%\n",
      "Epoch 100: Loss = 0.6924810097283001, Accuracy = 51.77%\n",
      "Epoch 200: Loss = 0.6923632564757726, Accuracy = 51.77%\n",
      "Epoch 300: Loss = 0.6921573640763262, Accuracy = 51.77%\n",
      "Epoch 400: Loss = 0.6917250808366587, Accuracy = 51.77%\n",
      "Epoch 500: Loss = 0.6908074167676949, Accuracy = 51.77%\n",
      "Epoch 600: Loss = 0.6888703905370681, Accuracy = 51.77%\n",
      "Epoch 700: Loss = 0.6847995957659427, Accuracy = 51.77%\n",
      "Epoch 800: Loss = 0.6763488311519362, Accuracy = 54.58%\n",
      "Epoch 900: Loss = 0.6592872855392184, Accuracy = 75.84%\n",
      "Epoch 999: Loss = 0.6270542090362031, Accuracy = 90.60%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       422\n",
      "           1       1.00      0.80      0.89       390\n",
      "\n",
      "    accuracy                           0.90       812\n",
      "   macro avg       0.92      0.90      0.90       812\n",
      "weighted avg       0.92      0.90      0.90       812\n",
      "\n",
      "[[421   1]\n",
      " [ 79 311]]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Treino e Avaliação\n",
    "# ==============================================\n",
    "\n",
    "# Treinar Rede Neural\n",
    "print(\"\\nTraining Neural Network...\")\n",
    "nn = NeuralNetwork(input_size=X_train.shape[1], hidden_size=64, \n",
    "                   output_size=1, lr=0.08)\n",
    "nn.train(X_train, y_train, epochs=1000)\n",
    "nn_pred = nn.predict(X_test)\n",
    "print(classification_report(y_test,nn_pred))\n",
    "print(confusion_matrix(y_test,nn_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
