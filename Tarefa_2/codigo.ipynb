{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Pré-processamento com Pandas/Scikit-learn\n",
    "# ==============================================\n",
    "\n",
    "# Carregar e combinar datasets\n",
    "#inputs_df = pd.read_csv('Dataset6_clean_input.csv')\n",
    "#outputs_df = pd.read_csv('Dataset6_clean_output.csv')\n",
    "#df = pd.merge(inputs_df, outputs_df, on='ID')\n",
    "df = pd.read_csv('../Tarefa_1/Dataset6_clean.csv' , sep=',')\n",
    "# Vetorização do texto\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Text']).toarray()\n",
    "y = df['Label'].map({'AI':1, 'Human':0}).values\n",
    "\n",
    "# Divisão treino-teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Modelos Implementados com Numpy\n",
    "# ==============================================\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(linear)\n",
    "            \n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self._sigmoid(linear)\n",
    "        return (y_pred > 0.5).astype(int)\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.01, dropout_rate=0.0):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.lr = lr\n",
    "        self.dropout_rate = dropout_rate  # Fraction of neurons to drop (0.0 means no dropout)\n",
    "        self.loss_history = []  # To monitor loss over epochs\n",
    "\n",
    "    def _relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def _relu_deriv(self, Z):\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    def _sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        # First layer forward\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self._relu(self.z1)\n",
    "        \n",
    "        # Apply dropout during training\n",
    "        if training and self.dropout_rate > 0.0:\n",
    "            # Create dropout mask: 1 means keep, 0 means drop.\n",
    "            self.dropout_mask = (np.random.rand(*self.a1.shape) > self.dropout_rate).astype(float)\n",
    "            # Apply inverted dropout scaling so that no scaling is needed at test time.\n",
    "            self.a1 *= self.dropout_mask\n",
    "            self.a1 /= (1.0 - self.dropout_rate)\n",
    "        else:\n",
    "            # If not training, no dropout mask is used.\n",
    "            self.dropout_mask = None\n",
    "        \n",
    "        # Second layer forward\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.output = self._sigmoid(self.z2)\n",
    "        return self.output\n",
    "\n",
    "    def compute_loss(self, y, output):\n",
    "        # Binary cross-entropy loss with a small epsilon for numerical stability\n",
    "        epsilon = 1e-15\n",
    "        y = y.reshape(-1, 1)\n",
    "        loss = -np.mean(y * np.log(output + epsilon) + (1 - y) * np.log(1 - output + epsilon))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        m = y.shape[0]\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Compute gradients for the output layer\n",
    "        dz2 = output - y\n",
    "        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Backpropagate to the hidden layer\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self._relu_deriv(self.z1)\n",
    "        # If dropout was applied, propagate the mask\n",
    "        if self.dropout_rate > 0.0 and self.dropout_mask is not None:\n",
    "            dz1 *= self.dropout_mask\n",
    "        \n",
    "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def train(self, X, y, epochs=1000, print_loss=True):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass with dropout enabled\n",
    "            output = self.forward(X, training=True)\n",
    "            \n",
    "            # Compute and store loss\n",
    "            loss = self.compute_loss(y, output)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Optionally print loss and accuracy at intervals\n",
    "            if print_loss and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                predictions = self.predict(X)\n",
    "                accuracy = np.mean(predictions.flatten() == y.flatten())\n",
    "                print(f\"Epoch {epoch}: Loss = {loss}, Accuracy = {accuracy*100:.2f}%\")\n",
    "            \n",
    "            # Backward pass and weight update\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y, output)\n",
    "            self.W1 -= self.lr * dW1\n",
    "            self.b1 -= self.lr * db1\n",
    "            self.W2 -= self.lr * dW2\n",
    "            self.b2 -= self.lr * db2\n",
    "\n",
    "    def predict(self, X):\n",
    "        # During prediction, disable dropout by setting training=False\n",
    "        output = self.forward(X, training=False)\n",
    "        return (output > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       422\n",
      "           1       0.99      0.91      0.95       390\n",
      "\n",
      "    accuracy                           0.95       812\n",
      "   macro avg       0.96      0.95      0.95       812\n",
      "weighted avg       0.96      0.95      0.95       812\n",
      "\n",
      "[[420   2]\n",
      " [ 36 354]]\n",
      "LR Test Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Treinar Regressão Logística\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr = LogisticRegression(lr=0.08, n_iters=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_acc = np.mean(lr.predict(X_test) == y_test)\n",
    "lr_pred = lr.predict(X_test)\n",
    "print(classification_report(y_test,lr_pred))\n",
    "print(confusion_matrix(y_test,lr_pred))\n",
    "print(f\"LR Test Accuracy: {lr_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Neural Network...\n",
      "Epoch 0: Loss = 0.6931657809290036, Accuracy = 48.54%\n",
      "Epoch 100: Loss = 0.6924918930890622, Accuracy = 51.77%\n",
      "Epoch 200: Loss = 0.6923538583358972, Accuracy = 51.77%\n",
      "Epoch 300: Loss = 0.6921849464101373, Accuracy = 51.77%\n",
      "Epoch 400: Loss = 0.6919018121813624, Accuracy = 51.77%\n",
      "Epoch 500: Loss = 0.6914080607054636, Accuracy = 51.77%\n",
      "Epoch 600: Loss = 0.6905459933225684, Accuracy = 51.77%\n",
      "Epoch 700: Loss = 0.6890428521479208, Accuracy = 51.77%\n",
      "Epoch 800: Loss = 0.6864312542372709, Accuracy = 51.77%\n",
      "Epoch 900: Loss = 0.6819155324579333, Accuracy = 51.77%\n",
      "Epoch 1000: Loss = 0.6741938597518515, Accuracy = 55.04%\n",
      "Epoch 1100: Loss = 0.661234140692196, Accuracy = 68.54%\n",
      "Epoch 1200: Loss = 0.6401286512042462, Accuracy = 82.87%\n",
      "Epoch 1300: Loss = 0.6073505800755, Accuracy = 90.97%\n",
      "Epoch 1400: Loss = 0.5600233966494584, Accuracy = 94.36%\n",
      "Epoch 1500: Loss = 0.4984569209776163, Accuracy = 96.36%\n",
      "Epoch 1600: Loss = 0.42829045107205577, Accuracy = 97.29%\n",
      "Epoch 1700: Loss = 0.35877438845646265, Accuracy = 97.87%\n",
      "Epoch 1800: Loss = 0.29761548476564026, Accuracy = 98.09%\n",
      "Epoch 1900: Loss = 0.24787775864171033, Accuracy = 98.34%\n",
      "Epoch 2000: Loss = 0.20897227645891517, Accuracy = 98.61%\n",
      "Epoch 2100: Loss = 0.178868201632376, Accuracy = 98.71%\n",
      "Epoch 2200: Loss = 0.155450597007099, Accuracy = 98.77%\n",
      "Epoch 2300: Loss = 0.1369948395069672, Accuracy = 98.86%\n",
      "Epoch 2400: Loss = 0.1222143612685721, Accuracy = 98.89%\n",
      "Epoch 2500: Loss = 0.11018229120777645, Accuracy = 98.92%\n",
      "Epoch 2600: Loss = 0.10023303063915079, Accuracy = 98.92%\n",
      "Epoch 2700: Loss = 0.09188696481320126, Accuracy = 99.01%\n",
      "Epoch 2800: Loss = 0.08479382977280779, Accuracy = 99.20%\n",
      "Epoch 2900: Loss = 0.0786948412079928, Accuracy = 99.29%\n",
      "Epoch 3000: Loss = 0.07339582366641884, Accuracy = 99.41%\n",
      "Epoch 3100: Loss = 0.06874878917417566, Accuracy = 99.45%\n",
      "Epoch 3200: Loss = 0.06463931369510213, Accuracy = 99.45%\n",
      "Epoch 3300: Loss = 0.060978066872735624, Accuracy = 99.51%\n",
      "Epoch 3400: Loss = 0.05769420749424774, Accuracy = 99.51%\n",
      "Epoch 3500: Loss = 0.05473112848660887, Accuracy = 99.54%\n",
      "Epoch 3600: Loss = 0.05204281395768659, Accuracy = 99.54%\n",
      "Epoch 3700: Loss = 0.04959173970891643, Accuracy = 99.54%\n",
      "Epoch 3800: Loss = 0.04734686324928892, Accuracy = 99.54%\n",
      "Epoch 3900: Loss = 0.04528244116867293, Accuracy = 99.54%\n",
      "Epoch 4000: Loss = 0.04337688913830709, Accuracy = 99.54%\n",
      "Epoch 4100: Loss = 0.04161191160637772, Accuracy = 99.57%\n",
      "Epoch 4200: Loss = 0.0399719720206201, Accuracy = 99.63%\n",
      "Epoch 4300: Loss = 0.03844380457819802, Accuracy = 99.66%\n",
      "Epoch 4400: Loss = 0.03701599224069296, Accuracy = 99.66%\n",
      "Epoch 4500: Loss = 0.03567868049203044, Accuracy = 99.66%\n",
      "Epoch 4600: Loss = 0.034423297880237964, Accuracy = 99.69%\n",
      "Epoch 4700: Loss = 0.03324235443205487, Accuracy = 99.69%\n",
      "Epoch 4800: Loss = 0.03212928785264195, Accuracy = 99.69%\n",
      "Epoch 4900: Loss = 0.031078313362974733, Accuracy = 99.69%\n",
      "Epoch 5000: Loss = 0.030084336387727634, Accuracy = 99.69%\n",
      "Epoch 5100: Loss = 0.029142804505377935, Accuracy = 99.69%\n",
      "Epoch 5200: Loss = 0.028249680744625014, Accuracy = 99.69%\n",
      "Epoch 5300: Loss = 0.027401338642876416, Accuracy = 99.72%\n",
      "Epoch 5400: Loss = 0.026594538722348567, Accuracy = 99.75%\n",
      "Epoch 5500: Loss = 0.0258263752924052, Accuracy = 99.75%\n",
      "Epoch 5600: Loss = 0.02509421579511634, Accuracy = 99.75%\n",
      "Epoch 5700: Loss = 0.024395664690266432, Accuracy = 99.75%\n",
      "Epoch 5800: Loss = 0.02372856394462069, Accuracy = 99.78%\n",
      "Epoch 5900: Loss = 0.023090941724750392, Accuracy = 99.78%\n",
      "Epoch 6000: Loss = 0.02248099471480895, Accuracy = 99.78%\n",
      "Epoch 6100: Loss = 0.021897068264637786, Accuracy = 99.82%\n",
      "Epoch 6200: Loss = 0.021337643814980215, Accuracy = 99.82%\n",
      "Epoch 6300: Loss = 0.020801327085442422, Accuracy = 99.85%\n",
      "Epoch 6400: Loss = 0.020286829701921753, Accuracy = 99.85%\n",
      "Epoch 6500: Loss = 0.01979295383520186, Accuracy = 99.85%\n",
      "Epoch 6600: Loss = 0.019318585801288417, Accuracy = 99.85%\n",
      "Epoch 6700: Loss = 0.018862694094127355, Accuracy = 99.85%\n",
      "Epoch 6800: Loss = 0.01842431969703979, Accuracy = 99.85%\n",
      "Epoch 6900: Loss = 0.018002564716493755, Accuracy = 99.88%\n",
      "Epoch 7000: Loss = 0.01759658849821794, Accuracy = 99.88%\n",
      "Epoch 7100: Loss = 0.017205604932086845, Accuracy = 99.88%\n",
      "Epoch 7200: Loss = 0.016828878498194887, Accuracy = 99.88%\n",
      "Epoch 7300: Loss = 0.01646571652941773, Accuracy = 99.88%\n",
      "Epoch 7400: Loss = 0.016115469553139194, Accuracy = 99.88%\n",
      "Epoch 7500: Loss = 0.01577752970963789, Accuracy = 99.88%\n",
      "Epoch 7600: Loss = 0.015451319296355601, Accuracy = 99.88%\n",
      "Epoch 7700: Loss = 0.015136293943784799, Accuracy = 99.91%\n",
      "Epoch 7800: Loss = 0.01483194242700796, Accuracy = 99.91%\n",
      "Epoch 7900: Loss = 0.0145377814716071, Accuracy = 99.94%\n",
      "Epoch 8000: Loss = 0.014253351614892953, Accuracy = 99.94%\n",
      "Epoch 8100: Loss = 0.013978223187873187, Accuracy = 99.97%\n",
      "Epoch 8200: Loss = 0.01371198752138399, Accuracy = 99.97%\n",
      "Epoch 8300: Loss = 0.013454255879984247, Accuracy = 99.97%\n",
      "Epoch 8400: Loss = 0.013204662507832551, Accuracy = 99.97%\n",
      "Epoch 8500: Loss = 0.012962860202398748, Accuracy = 99.97%\n",
      "Epoch 8600: Loss = 0.012728520808347875, Accuracy = 99.97%\n",
      "Epoch 8700: Loss = 0.012501331816175859, Accuracy = 99.97%\n",
      "Epoch 8800: Loss = 0.012280995412310043, Accuracy = 100.00%\n",
      "Epoch 8900: Loss = 0.012067230172201578, Accuracy = 100.00%\n",
      "Epoch 9000: Loss = 0.01185977065540287, Accuracy = 100.00%\n",
      "Epoch 9100: Loss = 0.01165836326440015, Accuracy = 100.00%\n",
      "Epoch 9200: Loss = 0.011462766077995104, Accuracy = 100.00%\n",
      "Epoch 9300: Loss = 0.011272749951651968, Accuracy = 100.00%\n",
      "Epoch 9400: Loss = 0.01108809726582724, Accuracy = 100.00%\n",
      "Epoch 9500: Loss = 0.010908600077077694, Accuracy = 100.00%\n",
      "Epoch 9600: Loss = 0.010734060873124032, Accuracy = 100.00%\n",
      "Epoch 9700: Loss = 0.010564291426388414, Accuracy = 100.00%\n",
      "Epoch 9800: Loss = 0.010399111672404182, Accuracy = 100.00%\n",
      "Epoch 9900: Loss = 0.010238351115178767, Accuracy = 100.00%\n",
      "Epoch 9999: Loss = 0.010083391174052837, Accuracy = 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       422\n",
      "           1       0.99      0.99      0.99       390\n",
      "\n",
      "    accuracy                           0.99       812\n",
      "   macro avg       0.99      0.99      0.99       812\n",
      "weighted avg       0.99      0.99      0.99       812\n",
      "\n",
      "[[420   2]\n",
      " [  5 385]]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Treino e Avaliação\n",
    "# ==============================================\n",
    "\n",
    "# Treinar Rede Neural\n",
    "print(\"\\nTraining Neural Network...\")\n",
    "nn = NeuralNetwork(input_size=X_train.shape[1], hidden_size=64, \n",
    "                   output_size=1, lr=0.06)\n",
    "nn.train(X_train, y_train, epochs=10000)\n",
    "nn_pred = nn.predict(X_test)\n",
    "print(classification_report(y_test,nn_pred))\n",
    "print(confusion_matrix(y_test,nn_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
