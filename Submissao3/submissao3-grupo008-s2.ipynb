{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissao few shots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils da pesquisa semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_dataset(dataset_version ):\n",
    "    base_path = f'../datasets/{dataset_version}/'\n",
    "    train_file = f'Dataset{dataset_version}_train_clean.csv'\n",
    "    test_file = f'Dataset{dataset_version}_test_clean.csv'\n",
    "    validation_file = f'Dataset{dataset_version}_validation_clean.csv'\n",
    "    \n",
    "    train = pd.read_csv(os.path.join(base_path, train_file), sep=',')\n",
    "    test = pd.read_csv(os.path.join(base_path, test_file), sep=',')\n",
    "    \n",
    "\n",
    "    # Verifica se o arquivo de validação existe\n",
    "    validation_path = os.path.join(base_path, validation_file)\n",
    "    if os.path.exists(validation_path):\n",
    "        validation = pd.read_csv(validation_path, sep=',')\n",
    "        df = pd.concat([train,test,validation])\n",
    "    else:\n",
    "        df = pd.concat([train,test])\n",
    "\n",
    "    return df\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def fill_db(vector_store, dataset_list_id ):\n",
    "\n",
    "    for dataset_id in dataset_list_id:\n",
    "        df = load_dataset(dataset_id)\n",
    "        N = len(df)\n",
    "        documents = [None] * N\n",
    "        print(f\"Loading dataset {dataset_id}\\n\")\n",
    "        for row in range(N):\n",
    "            documents[row] = Document(page_content=df['text'].iloc[row], metadata={\"label\" : df['Label'].iloc[row]})\n",
    "\n",
    "        # 4min for 20k rows\n",
    "        vector_store.add_documents(documents)\n",
    "\n",
    "# Formatar em csv os exemplos obtidos na query da db\n",
    "def formatExamples(results):\n",
    "    df = pd.DataFrame(columns=['Label','Text'])\n",
    "\n",
    "    for i,doc in enumerate(results):\n",
    "        df.loc[i] = [doc.metadata['label'],doc.page_content]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# vector_store.similarity_search(query=e1,k=2,filter={\"label\": \"AI\"})\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 3\n",
      "\n",
      "Loading dataset 5\n",
      "\n",
      "Loading dataset 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"human_vs_ai\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "fill_db(vector_store,[3,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils de API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import anthropic\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    ")\n",
    "\n",
    "# gemini-embedding-exp\n",
    "client_google = genai.Client(\n",
    "    api_key= os.environ.get(\"GEMINI_API_KEY\")\n",
    "    )\n",
    "\n",
    "PROVIDER_ANTHROPIC = \"anthropic\"\n",
    "PROVIDER_GOOGLE = \"google\"\n",
    "\n",
    "def classify_texts_with_shots(vector_store,shots,shotsPerLabel,df_input, output_csv_path, batch_size=10, model=\"claude-3-7-sonnet-20250219\", provider=PROVIDER_ANTHROPIC):\n",
    "    \n",
    "    df = df_input.copy()\n",
    "    df['Label'] = ''\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert at identifying AI-generated text versus human-written text.\n",
    "    \n",
    "    Analyze each numbered text sample carefully and classify it as either 'Human' or 'AI' based on these criteria:\n",
    "    \n",
    "    Human-written text often:\n",
    "    - Contains personal anecdotes or emotional nuance\n",
    "    - Has natural irregularities, varying sentence structures\n",
    "    - May include idioms, slang, or colloquialisms \n",
    "    - Can have slight grammatical errors or typos\n",
    "    - Often has a distinctive voice or style\n",
    "    \n",
    "    AI-generated text often:\n",
    "    - Has more uniform sentence structures\n",
    "    - Uses more formal or academic language consistently\n",
    "    - Organizes information very systematically\n",
    "    - Rarely contains spelling errors or typos\n",
    "    - May have repetitive patterns or phrasing\n",
    "    \n",
    "    IMPORTANT: Return your analysis as a CSV format with two columns (ID,LABEL) where classification is ONLY 'Human' or 'AI'.\n",
    "    Do not include any other text in your response besides the CSV data.\n",
    "    Do not add a prefix such as csv.\n",
    "    Example output format:\n",
    "    ID,LABEL\n",
    "    1,Human\n",
    "    2,AI\n",
    "    3,Human\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process dataframe in batches\n",
    "    num_samples = len(df)\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size  # Ceiling division\n",
    "    \n",
    "    print(f\"Processing {num_samples} text samples in {num_batches} batches of size {batch_size}...\")\n",
    "    \n",
    "    for batch_idx in tqdm(range(num_batches)):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        examples_df = []\n",
    "        # Prepare the batch of texts to classify\n",
    "        batch_text = \"\"\n",
    "        for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            relative_idx = i + 1\n",
    "            text = row['Text']\n",
    "            batch_text += f\"Text {relative_idx}: {text}\\n\\n\"\n",
    "\n",
    "            if shotsPerLabel:\n",
    "                results_AI = vector_store.similarity_search(query= row['Text'],k=shots,filter={\"label\": \"AI\"})\n",
    "                results_HUMAN = vector_store.similarity_search(query= row['Text'],k=shots,filter={\"label\": \"Human\"})\n",
    "                examples_df.append( formatExamples(results_AI) )\n",
    "                examples_df.append( formatExamples(results_HUMAN) )\n",
    "            else:\n",
    "                results = vector_store.similarity_search(query= row['Text'],k=shots)\n",
    "                examples_df.append( formatExamples(results) )\n",
    "\n",
    "            \n",
    "        \n",
    "        shots_text = pd.concat(examples_df).to_csv(index=False)\n",
    "        # print(f\"\\nShots text: {shots_text}\\n\\n\")\n",
    "\n",
    "        # Prepare the user message\n",
    "        user_message = f\"First, I am providing some examples that may be helpful in classyfing texts as either 'Human' or 'AI', the examples are in csv and are the following:\\n\\n {shots_text}\\n\\n Please classify each of the following texts as either 'Human' or 'AI':\\n\\n{batch_text}\\n\\nReturn your analysis in CSV format with columns 'ID' and 'Label'.\"\n",
    "        \n",
    "        # print(f\"Prompting the following message: {user_message}\")\n",
    "\n",
    "        max_retries = 3\n",
    "        retry_delay = 2\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "\n",
    "                if provider == PROVIDER_ANTHROPIC:\n",
    "                    response = client.messages.create(\n",
    "                        model=model,\n",
    "                        system=system_prompt,\n",
    "                        max_tokens=2000,  \n",
    "                        messages=[\n",
    "                            {\"role\": \"user\", \"content\": user_message}\n",
    "                        ]\n",
    "                    )\n",
    "                    \n",
    "                    csv_response = response.content[0].text.strip()\n",
    "                else: # google\n",
    "                    \n",
    "                    response = client_google.models.generate_content(\n",
    "                        model= model,\n",
    "                        contents= user_message,\n",
    "                        config=types.GenerateContentConfig(\n",
    "                            system_instruction = system_prompt #,\n",
    "                            # max_output_tokens=1000,\n",
    "                        )\n",
    "                    )\n",
    "                    # flash adds csv prefix.\n",
    "                    csv_response = response.text.removeprefix(\"```csv\\n\").removesuffix(\"```\").strip()\n",
    "\n",
    "                try:\n",
    "                    import io\n",
    "                    result_df = pd.read_csv(io.StringIO(csv_response))\n",
    "                    \n",
    "                    for row in  range(len(result_df)):\n",
    "                        relative_idx = result_df.iloc[row,0]\n",
    "                        classification = result_df.iloc[row,1]\n",
    "                                                \n",
    "                        classification = classification.upper()\n",
    "\n",
    "                        if classification == 'HUMAN':\n",
    "                            classification = 'Human'\n",
    "\n",
    "                        abs_idx = start_idx + relative_idx - 1\n",
    "                        \n",
    "                        if abs_idx < end_idx: \n",
    "                            df.at[abs_idx, 'Label'] = classification\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing CSV response: {e}\")\n",
    "                    print(f\"Raw response: {csv_response}\")\n",
    "\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on attempt {attempt+1}: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                else:\n",
    "                    df.loc[start_idx:end_idx-1, 'Label'] = 'error'\n",
    "                    print(f\"Failed to classify batch after {max_retries} attempts.\")\n",
    "        \n",
    "        # Add a small delay between batches to respect rate limits\n",
    "        time.sleep(1)\n",
    "    \n",
    "    df = df[['ID','Label']]\n",
    "    df.set_index('ID', inplace=True)\n",
    "    df.to_csv(output_csv_path, index=True, sep='\\t')\n",
    "    print(f\"Classification complete. Results saved to {output_csv_path}\")\n",
    "    \n",
    "    # Return summary statistics\n",
    "    human_count = (df['Label'] == 'Human').sum()\n",
    "    ai_count = (df['Label'] == 'AI').sum()\n",
    "    other_count = len(df) - human_count - ai_count\n",
    "    \n",
    "    print(f\"Summary:\\n- Human texts: {human_count}\\n- AI texts: {ai_count}\\n- Other/errors: {other_count}\")\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv('dataset2_disclosed_complete.csv', sep=';')\n",
    "df_test['Label'] = df_test['Label'].astype(str)\n",
    "df_test.loc[df_test['Label'] == 'Ai', 'Label'] = \"AI\"\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def test_model(y_pred):\n",
    "    y_pred = y_pred.astype(str) \n",
    "    print(classification_report(df_test['Label'], y_pred))\n",
    "    print(confusion_matrix(df_test['Label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes para escolher o melhor numero de shots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k = 1 , one shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 1 exemplos mais parecidos com cada row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 text samples in 10 batches of size 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:06<00:00,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification complete. Results saved to test_claude_k1_m0.csv\n",
      "Summary:\n",
      "- Human texts: 41\n",
      "- AI texts: 59\n",
      "- Other/errors: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AI       0.80      0.96      0.87        49\n",
      "       Human       0.95      0.76      0.85        51\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.87      0.86      0.86       100\n",
      "weighted avg       0.88      0.86      0.86       100\n",
      "\n",
      "[[47  2]\n",
      " [12 39]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_claude_k1_m0 = classify_texts_with_shots(vector_store,1,False,df_test, \"test_claude_k1_m0.csv\", batch_size=10) # .12\n",
    "test_model(test_claude_k1_m0['Label']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K=1 exemplos mais parecidos com cada row e label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 text samples in 10 batches of size 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:39<00:00, 21.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification complete. Results saved to test_claude_k1_m2.csv\n",
      "Summary:\n",
      "- Human texts: 47\n",
      "- AI texts: 53\n",
      "- Other/errors: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AI       0.87      0.94      0.90        49\n",
      "       Human       0.94      0.86      0.90        51\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n",
      "[[46  3]\n",
      " [ 7 44]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#def classify_texts_with_shots(vector_store,shots,shotsPerLabel,df_input, output_csv_path, batch_size=10, model=\"claude-3-7-sonnet-20250219\", provider=\"anthropic\"):\n",
    "\n",
    "test_claude_k1_m2 = classify_texts_with_shots(vector_store,1,True,df_test, \"test_claude_k1_m2.csv\", batch_size=10) \n",
    "test_model(test_claude_k1_m2['Label']) #.20 cents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K=3 exemplos mais parecidos com cada row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 text samples in 20 batches of size 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:27<00:00, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification complete. Results saved to test_claude_k3_m0.csv\n",
      "Summary:\n",
      "- Human texts: 39\n",
      "- AI texts: 61\n",
      "- Other/errors: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AI       0.79      0.98      0.87        49\n",
      "       Human       0.97      0.75      0.84        51\n",
      "\n",
      "    accuracy                           0.86       100\n",
      "   macro avg       0.88      0.86      0.86       100\n",
      "weighted avg       0.88      0.86      0.86       100\n",
      "\n",
      "[[48  1]\n",
      " [13 38]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_claude_k3_m0 = classify_texts_with_shots(vector_store,3,False,df_test, \"test_claude_k3_m0.csv\", batch_size=5) # 0.30 \n",
    "test_model(test_claude_k3_m0['Label']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K=3 exemplos mais parecidos com cada row e label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 text samples in 20 batches of size 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [07:15<00:00, 21.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification complete. Results saved to test_claude_k3_m2.csv\n",
      "Summary:\n",
      "- Human texts: 42\n",
      "- AI texts: 58\n",
      "- Other/errors: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AI       0.81      0.96      0.88        49\n",
      "       Human       0.95      0.78      0.86        51\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.88      0.87      0.87       100\n",
      "weighted avg       0.88      0.87      0.87       100\n",
      "\n",
      "[[47  2]\n",
      " [11 40]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_claude_k3_m2 = classify_texts_with_shots(vector_store,3,True,df_test, \"test_claude_k3_m2.csv\", batch_size=5) # ,model = \"claude-3-haiku-20240307\"\n",
    "test_model(test_claude_k3_m2['Label']) # 60cents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissão final\n",
    "\n",
    "k = 1, o exemplo mais parecido para cada row e label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 text samples in 10 batches of size 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:27<00:00, 14.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification complete. Results saved to submissao3-grupo008-s2.csv\n",
      "Summary:\n",
      "- Human texts: 42\n",
      "- AI texts: 58\n",
      "- Other/errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "5194209a-9aa5-4561-9aae-755ee5808248",
       "rows": [
        [
         "D3-1",
         "AI"
        ],
        [
         "D3-2",
         "AI"
        ],
        [
         "D3-3",
         "AI"
        ],
        [
         "D3-4",
         "Human"
        ],
        [
         "D3-5",
         "Human"
        ],
        [
         "D3-6",
         "AI"
        ],
        [
         "D3-7",
         "AI"
        ],
        [
         "D3-8",
         "AI"
        ],
        [
         "D3-9",
         "AI"
        ],
        [
         "D3-10",
         "AI"
        ],
        [
         "D3-11",
         "Human"
        ],
        [
         "D3-12",
         "AI"
        ],
        [
         "D3-13",
         "AI"
        ],
        [
         "D3-14",
         "AI"
        ],
        [
         "D3-15",
         "Human"
        ],
        [
         "D3-16",
         "Human"
        ],
        [
         "D3-17",
         "AI"
        ],
        [
         "D3-18",
         "Human"
        ],
        [
         "D3-19",
         "Human"
        ],
        [
         "D3-20",
         "Human"
        ],
        [
         "D3-21",
         "AI"
        ],
        [
         "D3-22",
         "Human"
        ],
        [
         "D3-23",
         "AI"
        ],
        [
         "D3-24",
         "AI"
        ],
        [
         "D3-25",
         "AI"
        ],
        [
         "D3-26",
         "Human"
        ],
        [
         "D3-27",
         "Human"
        ],
        [
         "D3-28",
         "AI"
        ],
        [
         "D3-29",
         "AI"
        ],
        [
         "D3-30",
         "AI"
        ],
        [
         "D3-31",
         "AI"
        ],
        [
         "D3-32",
         "AI"
        ],
        [
         "D3-33",
         "Human"
        ],
        [
         "D3-34",
         "AI"
        ],
        [
         "D3-35",
         "AI"
        ],
        [
         "D3-36",
         "Human"
        ],
        [
         "D3-37",
         "AI"
        ],
        [
         "D3-38",
         "Human"
        ],
        [
         "D3-39",
         "Human"
        ],
        [
         "D3-40",
         "AI"
        ],
        [
         "D3-41",
         "AI"
        ],
        [
         "D3-42",
         "Human"
        ],
        [
         "D3-43",
         "AI"
        ],
        [
         "D3-44",
         "Human"
        ],
        [
         "D3-45",
         "AI"
        ],
        [
         "D3-46",
         "AI"
        ],
        [
         "D3-47",
         "Human"
        ],
        [
         "D3-48",
         "AI"
        ],
        [
         "D3-49",
         "Human"
        ],
        [
         "D3-50",
         "AI"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 100
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D3-1</th>\n",
       "      <td>AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3-2</th>\n",
       "      <td>AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3-3</th>\n",
       "      <td>AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3-4</th>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3-5</th>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3-96</th>\n",
       "      <td>AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3-97</th>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3-98</th>\n",
       "      <td>AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3-99</th>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3-100</th>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Label\n",
       "ID           \n",
       "D3-1       AI\n",
       "D3-2       AI\n",
       "D3-3       AI\n",
       "D3-4    Human\n",
       "D3-5    Human\n",
       "...       ...\n",
       "D3-96      AI\n",
       "D3-97   Human\n",
       "D3-98      AI\n",
       "D3-99   Human\n",
       "D3-100  Human\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_input = pd.read_csv('submission3_inputs.csv', sep=';')\n",
    "\n",
    "classify_texts_with_shots(vector_store,1,True,df_input, \"submissao3-grupo008-s2.csv\", batch_size=10) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
