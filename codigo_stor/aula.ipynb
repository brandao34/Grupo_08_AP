{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.45704064 -0.02902311 -0.8384594 ]\n",
      "  [-0.75259094 -0.11658635 -1.28079961]\n",
      "  [-0.84365269 -0.16970949 -1.40036099]]\n",
      "\n",
      " [[-0.45704064 -0.02902311 -0.8384594 ]\n",
      "  [-0.75259094 -0.11658635 -1.28079961]\n",
      "  [-0.84365269 -0.16970949 -1.40036099]]]\n",
      "[[[ 0  0  0]\n",
      "  [-1  0  0]\n",
      "  [ 0  1  0]]\n",
      "\n",
      " [[ 0  0  0]\n",
      "  [-1  0  0]\n",
      "  [ 0  1  0]]]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from si.neural_networks.activation import TanhActivation, ActivationLayer\n",
    "from si.neural_networks.layers import Layer\n",
    "from si.neural_networks.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class RNN(Layer):\n",
    "    \"\"\"A Vanilla Fully-Connected Recurrent Neural Network layer.\"\"\"\n",
    "\n",
    "    def __init__(self, n_units: int, activation: ActivationLayer = None, bptt_trunc: int = 5,\n",
    "                 input_shape: Tuple = None):\n",
    "        \"\"\"\n",
    "        Initializes the layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_units: int\n",
    "            The number of units in the layer (i.e. the number of hidden states).\n",
    "        activation: ActivationLayer\n",
    "            The activation function to apply to the output of each state.\n",
    "        bptt_trunc: int\n",
    "            The number of time steps to backpropagate through time (i.e. the number of time steps to unroll the RNN).\n",
    "        input_shape: Tuple\n",
    "            The shape of the input to the layer.\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.n_units = n_units\n",
    "        self.activation = TanhActivation() if activation is None else activation\n",
    "        self.bptt_trunc = bptt_trunc\n",
    "\n",
    "        self.W = None  # Weight of the previous state\n",
    "        self.V = None  # Weight of the output\n",
    "        self.U = None  # Weight of the input\n",
    "\n",
    "    def initialize(self, optimizer):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer: Optimizer\n",
    "            The optimizer to use for updating the weights.\n",
    "        \"\"\"\n",
    "        timesteps, input_dim = self.input_shape\n",
    "        # Initialize the weights\n",
    "        limit = 1 / np.sqrt(input_dim)\n",
    "        self.U = np.random.uniform(-limit, limit, (self.n_units, input_dim))\n",
    "        limit = 1 / np.sqrt(self.n_units)\n",
    "        self.V = np.random.uniform(-limit, limit, (input_dim, self.n_units))\n",
    "        self.W = np.random.uniform(-limit, limit, (self.n_units, self.n_units))\n",
    "        # Weight optimizers\n",
    "        self.U_opt = deepcopy(optimizer)\n",
    "        self.V_opt = deepcopy(optimizer)\n",
    "        self.W_opt = deepcopy(optimizer)\n",
    "\n",
    "    def forward_propagation(self, input: np.ndarray, training: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform forward propagation on the given input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input: numpy.ndarray\n",
    "            The input to the layer.\n",
    "        training: bool\n",
    "            Whether the layer is in training mode or in inference mode.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            The output of the layer.\n",
    "        \"\"\"\n",
    "        self.layer_input = input\n",
    "        batch_size, timesteps, input_dim = input.shape\n",
    "\n",
    "        # Save these values for use in backprop.\n",
    "        self.state_input = np.zeros((batch_size, timesteps, self.n_units))\n",
    "        self.states = np.zeros((batch_size, timesteps + 1, self.n_units))\n",
    "        self.outputs = np.zeros((batch_size, timesteps, input_dim))\n",
    "\n",
    "        # Set last time step to zero for calculation of the state_input at time step zero (already zero?)\n",
    "        # self.states[:, -1] = np.zeros((batch_size, self.n_units))\n",
    "        for t in range(timesteps):\n",
    "            # Input to state_t is the current input and output of previous states\n",
    "            self.state_input[:, t] = input[:, t].dot(self.U.T) + self.states[:, t - 1].dot(self.W.T)\n",
    "            self.states[:, t] = self.activation.activation_function(self.state_input[:, t])\n",
    "            self.outputs[:, t] = self.states[:, t].dot(self.V.T)\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    def backward_propagation(self, accum_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform backward propagation on the given output error.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        accum_grad: numpy.ndarray\n",
    "            The accumulated gradient from the previous layer.\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            The accumulated gradient w.r.t the input of the layer.\n",
    "        \"\"\"\n",
    "        _, timesteps, _ = accum_grad.shape\n",
    "\n",
    "        # Variables where we save the accumulated gradient w.r.t each parameter\n",
    "        grad_U = np.zeros_like(self.U)\n",
    "        grad_V = np.zeros_like(self.V)\n",
    "        grad_W = np.zeros_like(self.W)\n",
    "        # The gradient w.r.t the layer input.\n",
    "        # Will be passed on to the previous layer in the network\n",
    "        accum_grad_next = np.zeros_like(accum_grad)\n",
    "\n",
    "        # Back Propagation Through Time\n",
    "        for t in reversed(range(timesteps)):\n",
    "            # Update gradient w.r.t V at time step t\n",
    "            grad_V += accum_grad[:, t].T.dot(self.states[:, t])\n",
    "            # Calculate the gradient w.r.t the state input\n",
    "            grad_wrt_state = accum_grad[:, t].dot(self.V) * self.activation.derivative(self.state_input[:, t])\n",
    "            # Gradient w.r.t the layer input\n",
    "            accum_grad_next[:, t] = grad_wrt_state.dot(self.U)\n",
    "            # Update gradient w.r.t W and U by backprop. from time step t for at most\n",
    "            # self.bptt_trunc number of time steps\n",
    "            for t_ in reversed(np.arange(max(0, t - self.bptt_trunc), t + 1)):\n",
    "                grad_U += grad_wrt_state.T.dot(self.layer_input[:, t_])\n",
    "                grad_W += grad_wrt_state.T.dot(self.states[:, t_ - 1])\n",
    "                # Calculate gradient w.r.t previous state\n",
    "                grad_wrt_state = grad_wrt_state.dot(self.W) * self.activation.derivative(self.state_input[:, t_ - 1])\n",
    "\n",
    "        # Update weights\n",
    "        self.U = self.U_opt.update(self.U, grad_U)\n",
    "        self.V = self.V_opt.update(self.V, grad_V)\n",
    "        self.W = self.W_opt.update(self.W, grad_W)\n",
    "\n",
    "        return accum_grad_next\n",
    "\n",
    "    def output_shape(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Returns the shape of the output of the layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            The shape of the output of the layer.\n",
    "        \"\"\"\n",
    "        return self.input_shape\n",
    "\n",
    "    def parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of parameters of the layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of parameters of the layer.\n",
    "        \"\"\"\n",
    "        return np.prod(self.W.shape) + np.prod(self.U.shape) + np.prod(self.V.shape)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n",
    "                     [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n",
    "    rnn = RNN(10, input_shape=(3, 3))\n",
    "    rnn.initialize(Optimizer())\n",
    "    print(rnn.forward_propagation(data))\n",
    "    print(rnn.backward_propagation(data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
