{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Pratico AP \n",
    "\n",
    "## Tarefa 1 Datasets\n",
    "\n",
    "\n",
    "1. https://huggingface.co/datasets/artem9k/ai-text-detection-pile \n",
    "2. https://huggingface.co/datasets/dmitva/human_ai_generated_text \n",
    "3. https://huggingface.co/datasets/artem9k/ai-text-detection-pile \n",
    "4. https://huggingface.co/datasets/NicolaiSivesind/human-vs-machine \n",
    "5. https://www.kaggle.com/datasets/prajwaldongre/llm-detect-ai-generated-vs-student-generated-text \n",
    "6. https://www.kaggle.com/datasets/heleneeriksen/gpt-vs-human-a-corpus-of-research-abstracts \n",
    "7. https://github.com/LorenzM97/human-AI-generatedTextCorpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo \n",
    "\n",
    "\n",
    "Tornar os varios datasets em 3 colunas: DatasetX_clean.csv\n",
    "\n",
    "| ID                             | Text          | Label       |\n",
    "| ------------------------------ | ------------- | ----------- |\n",
    "| D(ID Dataset) - (Linha Dataset) | EXEMPLO TEXT  | Human or IA |\n",
    "\n",
    "\n",
    "Dividir em:\n",
    "\n",
    "**DatasetX_clean_inputs**\n",
    "| ID                             | Text          |\n",
    "| ------------------------------ | ------------- | \n",
    "| D(ID Dataset) - (Linha Dataset) | EXEMPLO TEXT  | \n",
    "\n",
    "**DatasetX_clean_outputs** \n",
    "| ID                             |  Label       |\n",
    "| ------------------------------ |  ----------- |\n",
    "| D(ID Dataset) - (Linha Dataset) |  Human or IA |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORts \n",
    "\n",
    "import pandas as pd \n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  label  \\\n",
      "0  Study of coupling loss on bi-columnar BSCCO/Ag...      0   \n",
      "1  Study of coupling loss on bi-columnar BSCCO/Ag...      1   \n",
      "2  Weighted Solyanik estimates for the strong max...      0   \n",
      "3  Weighted Solyanik estimates for the strong max...      1   \n",
      "4  SOFIA-EXES Observations of Betelgeuse during t...      0   \n",
      "\n",
      "                                                text  word_count  \n",
      "0  Coupling losses were studied in composite tape...         280  \n",
      "1  In this study, we investigate the coupling los...         215  \n",
      "2  Let $\\mathsf M_{\\mathsf S}$ denote the strong ...         332  \n",
      "3  In this paper, we investigate Weighted Solyani...         225  \n",
      "4  In 2019 October Betelgeuse began a decline in ...         268  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"NicolaiSivesind/human-vs-machine\", \"research_abstracts_labeled\")\n",
    "\n",
    "# Concatenate all splits ('train', 'test', 'validation') into a single DataFrame\n",
    "df_train = pd.DataFrame(ds['train'])\n",
    "df_test = pd.DataFrame(ds['test'])\n",
    "df_validation = pd.DataFrame(ds['validation'])\n",
    "\n",
    "#df_train = pd.concat([df_train, df_validation], ignore_index=True)\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "df = pd.concat([df_train, df_test, df_validation], ignore_index=True)\n",
    "\n",
    "def process_dataset(split_name,df,number):\n",
    "    # Add the 'ID' column with format \"D4 - X\"\n",
    "    df['ID'] = ['D4 - ' + str(i + 1) for i in range(len(df))]\n",
    "\n",
    "    # Map the 'label' to 'Human' (0) or 'AI' (1)\n",
    "    df['Label'] = df['label'].map({0: 'Human', 1: 'AI'})\n",
    "\n",
    "    # Keep only the 'ID', 'text', and 'Label' columns\n",
    "    df = df[['ID', 'text', 'Label']]\n",
    "\n",
    "    # Save to CSV in the desired format\n",
    "    df.to_csv(f'3/Dataset{number}_{split_name}_clean.csv', index=False)\n",
    "\n",
    "# Optional: Print the first few rows of the resulting DataFrame to verify\n",
    "print(df.head())\n",
    "\n",
    "process_dataset('train',df_train,3)\n",
    "process_dataset('test',df_test,3)\n",
    "process_dataset('val',df_validation,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           title  label                                               text  \\\n",
      "0   James Scobie      0  James Scobie (29 November 1826 – 7 October 185...   \n",
      "1   James Scobie      1  James Scobie (29 November 1826 – 7 October 189...   \n",
      "2   Dagliç sheep      0  The Dagliç is a breed of sheep found primarily...   \n",
      "3   Dagliç sheep      1  The Dagliç is a breed of sheep that is found i...   \n",
      "4  Hamdard India      0  Hamdard Laboratories (India), is a Unani pharm...   \n",
      "\n",
      "   word_count  \n",
      "0         175  \n",
      "1         242  \n",
      "2         152  \n",
      "3          33  \n",
      "4         160  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"NicolaiSivesind/human-vs-machine\", \"wiki_labeled\")\n",
    "\n",
    "# Concatenate all splits ('train', 'test', 'validation') into a single DataFrame\n",
    "df_train = pd.DataFrame(ds['train'])\n",
    "df_test = pd.DataFrame(ds['test'])\n",
    "df_validation = pd.DataFrame(ds['validation'])\n",
    "\n",
    "#df_train = pd.concat([df_train, df_validation], ignore_index=True)\n",
    "\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "df = pd.concat([df_train, df_test, df_validation], ignore_index=True)\n",
    "\n",
    "def process_dataset(split_name,df,number):\n",
    "    # Add the 'ID' column with format \"D4 - X\"\n",
    "    df['ID'] = ['D4 - ' + str(i + 1) for i in range(len(df))]\n",
    "\n",
    "    # Map the 'label' to 'Human' (0) or 'AI' (1)\n",
    "    df['Label'] = df['label'].map({0: 'Human', 1: 'AI'})\n",
    "\n",
    "    # Keep only the 'ID', 'text', and 'Label' columns\n",
    "    df = df[['ID', 'text', 'Label']]\n",
    "\n",
    "    # Save to CSV in the desired format\n",
    "    df.to_csv(f'4/Dataset{number}_{split_name}_clean.csv', index=False)\n",
    "\n",
    "# Optional: Print the first few rows of the resulting DataFrame to verify\n",
    "print(df.head())\n",
    "\n",
    "process_dataset('train',df_train,4)\n",
    "process_dataset('test',df_test,4)\n",
    "process_dataset('validation',df_validation,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import os\n",
    "\n",
    "def process_dataset_5(input_file, output_file, dataset_id):\n",
    "    try:\n",
    "        # Lê o arquivo CSV\n",
    "        df = pd.read_csv(input_file)\n",
    "        \n",
    "        # Adiciona uma coluna de ID formatada (numerando a partir de 1)\n",
    "        df['ID'] = [f\"D{dataset_id} - {i}\" for i in range(1, len(df) + 1)]\n",
    "        df = df.rename(columns={'Text': 'text'})  # Renomeia a coluna\n",
    "        df['Label'] = df['Label'].map({'student': 'Human', 'ai': 'AI'})\n",
    "        \n",
    "        # Reorganiza as colunas para que o ID fique em primeiro\n",
    "        colunas = ['ID'] + [col for col in df.columns if col != 'ID']\n",
    "        df = df[colunas]\n",
    "\n",
    "        \n",
    "        \n",
    "        # --- NOVO: DIVISÃO EM TREINO E TESTE ---\n",
    "        # Divide o dataset (80% treino, 20% teste)\n",
    "        train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Gera nomes para os arquivos de saída\n",
    "        base_name = os.path.splitext(output_file)[0]  # Remove a extensão .csv\n",
    "        train_file = f\"5/Dataset5_train_clean.csv\"\n",
    "        test_file = f\"5/Dataset5_test_clean.csv\"\n",
    "        \n",
    "        # Salva os arquivos\n",
    "        train_df.to_csv(train_file, index=False)\n",
    "\n",
    "        test_df.to_csv(test_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "        # ----------------------------------------\n",
    "        \n",
    "        print(f\"CSVs gerados com sucesso: '{train_file}' e '{test_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSVs gerados com sucesso: '5/Dataset5_train_clean.csv' e '5/Dataset5_test_clean.csv'.\n"
     ]
    }
   ],
   "source": [
    "process_dataset_5( 'datasets_originais/LLM.csv', '5/Dataset5_clean.csv',5)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "def process_dataset_6(dataset_id, input_file, output_file):\n",
    "    \"\"\"\n",
    "    Processa um dataset CSV, adicionando uma coluna de ID, extraindo Abstract para Text e usando is_ai_generated como Label.\n",
    "    Agora também divide o dataset em treino (80%) e teste (20%).\n",
    "    \n",
    "    :param dataset_id: Identificador numérico do dataset\n",
    "    :param input_file: Caminho do arquivo de entrada CSV\n",
    "    :param output_file: Caminho do arquivo de saída CSV (base para treino e teste)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lê o arquivo CSV tratando quebras de linha dentro de campos de texto\n",
    "        df = pd.read_csv(input_file, quoting=csv.QUOTE_ALL, on_bad_lines='skip')\n",
    "        \n",
    "        # Adiciona uma coluna de ID formatada (numerando a partir de 1)\n",
    "        df['ID'] = [f\"D{dataset_id} - {i}\" for i in range(1, len(df) + 1)]\n",
    "        \n",
    "        # Mantém apenas as colunas necessárias\n",
    "        df = df[['ID', 'abstract', 'is_ai_generated']]\n",
    "        \n",
    "        # Renomeia colunas\n",
    "        df.rename(columns={'abstract': 'text', 'is_ai_generated': 'Label'}, inplace=True)\n",
    "        df['Label'] = df['Label'].map({0: 'Human', 1: 'AI'})\n",
    "\n",
    "        # --- NOVO: DIVISÃO EM TREINO E TESTE ---\n",
    "        # Divide o dataset (80% treino, 20% teste) mantendo proporção das classes\n",
    "        train_df, test_df = train_test_split(\n",
    "            df,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=df['Label']  # Mantém proporção de labels\n",
    "        )\n",
    "        \n",
    "        # Gera nomes para os arquivos de saída\n",
    "        base_name = os.path.splitext(output_file)[0]  # Remove a extensão .csv\n",
    "        train_file = f\"6/Dataset6_train_clean.csv\"\n",
    "        test_file = f\"6/Dataset6_test_clean.csv\"\n",
    "        \n",
    "        # Salva os arquivos\n",
    "        train_df.to_csv(train_file, index=False)\n",
    "        test_df.to_csv(test_file, index=False)\n",
    "        \n",
    "\n",
    "        print(f\"CSVs gerados com sucesso: '{train_file}' e '{test_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSVs gerados com sucesso: '6/Dataset6_train_clean.csv' e '6/Dataset6_test_clean.csv'.\n"
     ]
    }
   ],
   "source": [
    "process_dataset_6(6, 'datasets_originais/data_set.csv', '6/Dataset6_clean.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
