{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Pratico AP \n",
    "\n",
    "## Tarefa 1 Datasets\n",
    "\n",
    "\n",
    "1. https://huggingface.co/datasets/artem9k/ai-text-detection-pile \n",
    "2. https://huggingface.co/datasets/dmitva/human_ai_generated_text \n",
    "3. https://huggingface.co/datasets/artem9k/ai-text-detection-pile \n",
    "4. https://huggingface.co/datasets/NicolaiSivesind/human-vs-machine \n",
    "5. https://www.kaggle.com/datasets/prajwaldongre/llm-detect-ai-generated-vs-student-generated-text \n",
    "6. https://www.kaggle.com/datasets/heleneeriksen/gpt-vs-human-a-corpus-of-research-abstracts \n",
    "7. https://github.com/LorenzM97/human-AI-generatedTextCorpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo \n",
    "\n",
    "\n",
    "Tornar os varios datasets em 3 colunas: DatasetX_clean.csv\n",
    "\n",
    "| ID                             | Text          | Label       |\n",
    "| ------------------------------ | ------------- | ----------- |\n",
    "| D(ID Dataset) - (Linha Dataset) | EXEMPLO TEXT  | Human or IA |\n",
    "\n",
    "\n",
    "Dividir em:\n",
    "\n",
    "**DatasetX_clean_inputs**\n",
    "| ID                             | Text          |\n",
    "| ------------------------------ | ------------- | \n",
    "| D(ID Dataset) - (Linha Dataset) | EXEMPLO TEXT  | \n",
    "\n",
    "**DatasetX_clean_outputs** \n",
    "| ID                             |  Label       |\n",
    "| ------------------------------ |  ----------- |\n",
    "| D(ID Dataset) - (Linha Dataset) |  Human or IA |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORts \n",
    "\n",
    "import pandas as pd \n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_5(dataset_id, input_file, output_file):\n",
    "    \"\"\"\n",
    "    Processa um dataset CSV, adicionando uma coluna de ID, convertendo labels e salvando a versão limpa.\n",
    "    \n",
    "    :param dataset_id: Identificador numérico do dataset\n",
    "    :param input_file: Caminho do arquivo de entrada CSV\n",
    "    :param output_file: Caminho do arquivo de saída CSV\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lê o arquivo CSV\n",
    "        df = pd.read_csv(input_file)\n",
    "        \n",
    "        # Adiciona uma coluna de ID formatada (numerando a partir de 1)\n",
    "        df['ID'] = [f\"D{dataset_id} - {i}\" for i in range(1, len(df) + 1)]\n",
    "        \n",
    "        df['Label'] = df['Label'].map({'student': 'Human', 'ai': 'AI'})\n",
    "        \n",
    "        # Reorganiza as colunas para que o ID fique em primeiro\n",
    "        colunas = ['ID'] + [col for col in df.columns if col != 'ID']\n",
    "        df = df[colunas]\n",
    "        \n",
    "        # Converte o DataFrame para CSV\n",
    "        csv_output = df.to_csv(index=False)\n",
    "        \n",
    "        # Salva a tabela no arquivo especificado\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(csv_output)\n",
    "        \n",
    "        print(f\"CSV gerado com sucesso em '{output_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV gerado com sucesso em 'Dataset5_clean.csv'.\n"
     ]
    }
   ],
   "source": [
    "process_dataset_5(5, '../datasets_originais/LLM.csv', 'Dataset5_clean.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103, 3)\n",
      "       ID                                               Text  Label\n",
      "0  D5 - 1                   y r u always l8 to the meetings?  Human\n",
      "1  D5 - 2  The project team embraced a user-centric desig...     AI\n",
      "2  D5 - 3  i dont like dealing with risks, it's too stres...  Human\n",
      "3  D5 - 4   i dont worry about reliability, it's good enough  Human\n",
      "4  D5 - 5  i dont care about human-centered design, just ...  Human\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data5 = pd.read_csv('Dataset5_clean.csv')\n",
    "\n",
    "print(data5.shape)\n",
    "print(data5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_6(dataset_id, input_file, output_file):\n",
    "    \"\"\"\n",
    "    Processa um dataset CSV, adicionando uma coluna de ID, extraindo Abstract para Text e usando is_ai_generated como Label.\n",
    "    \n",
    "    :param dataset_id: Identificador numérico do dataset\n",
    "    :param input_file: Caminho do arquivo de entrada CSV\n",
    "    :param output_file: Caminho do arquivo de saída CSV\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lê o arquivo CSV tratando quebras de linha dentro de campos de texto\n",
    "        df = pd.read_csv(input_file, quoting=csv.QUOTE_ALL, on_bad_lines='skip')\n",
    "        \n",
    "        # Adiciona uma coluna de ID formatada (numerando a partir de 1)\n",
    "        df['ID'] = [f\"D{dataset_id} - {i}\" for i in range(1, len(df) + 1)]\n",
    "        \n",
    "        # Mantém apenas as colunas necessárias\n",
    "        df = df[['ID', 'abstract', 'is_ai_generated']]\n",
    "        \n",
    "        # Renomeia colunas\n",
    "        df.rename(columns={'abstract': 'Text', 'is_ai_generated': 'Label'}, inplace=True)\n",
    "        df['Label'] = df['Label'].map({0: 'Human', 1: 'AI'})\n",
    "\n",
    "        # Converte o DataFrame para CSV\n",
    "        csv_output = df.to_csv(index=False)\n",
    "        \n",
    "        # Salva a tabela no arquivo especificado\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(csv_output)\n",
    "        \n",
    "        print(f\"CSV gerado com sucesso em '{output_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o dataset: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV gerado com sucesso em 'Dataset6_clean.csv'.\n"
     ]
    }
   ],
   "source": [
    "process_dataset_6(6, '../datasets_originais/data_set.csv', 'Dataset6_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4057, 3)\n",
      "       ID                                               Text  Label\n",
      "0  D6 - 1    Advanced electromagnetic potentials are indi...  Human\n",
      "1  D6 - 2  This research paper investigates the question ...     AI\n",
      "2  D6 - 3    We give an algorithm for finding network enc...  Human\n",
      "3  D6 - 4  The paper presents an efficient centralized bi...     AI\n",
      "4  D6 - 5    Advanced electromagnetic potentials are indi...  Human\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data6 = pd.read_csv('Dataset6_clean.csv')\n",
    "\n",
    "print(data6.shape)\n",
    "print(data6.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(caminho_csv, nome_dataset=None):\n",
    "    # Lê o ficheiro CSV original\n",
    "    df = pd.read_csv(caminho_csv)\n",
    "    \n",
    "    # Seleciona as colunas para o ficheiro de input\n",
    "    df_input = df[['ID', 'Text']]\n",
    "    \n",
    "    # Seleciona as colunas para o ficheiro de output\n",
    "    df_output = df[['ID', 'Label']]\n",
    "    \n",
    "    # Escreve os dois novos ficheiros CSV\n",
    "    df_input.to_csv(f\"{nome_dataset}_input.csv\", index=False)\n",
    "    df_output.to_csv(f\"{nome_dataset}_output.csv\", index=False)\n",
    "    \n",
    "    print(\"Ficheiros gerados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ficheiros gerados com sucesso!\n",
      "Ficheiros gerados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "split_dataset('Dataset5_clean.csv', 'Dataset5_clean')\n",
    "\n",
    "split_dataset('Dataset6_clean.csv', 'Dataset6_clean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAA123",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
