{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot examplo manual\n",
    "\n",
    "Testamos a mesma prompt em 3 LLMs diferentes. Openai reason, R1 do deepseek e claude 3.7 Sonnet da anthropic.\n",
    "\n",
    "Prompt utilizada:\n",
    "    \" I am giving you a task of detecting if text is AI generated or Human generated. I will give you 30 examples and I want to classify it as 'Human' or 'AI'. I want the answer to be a list like ['AI,'AI'] etc.. so I can copy to python code. The examples to classify are the following! \"\n",
    "\n",
    "Para cada analisamos o resultado para descobrir o melhor modelo.\n",
    "\n",
    "No final repetimos a prompt para o melhor modelo com o dataset de submissão.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_openai = ['AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI',\n",
    " 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'Human', 'AI',\n",
    " 'AI', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human']\n",
    "\n",
    "# Prompt:\n",
    "# I am giving you a task of detecting if text is AI generated or Human generated. I will give you 30 examples and I want to classify it as 'Human' or 'AI'. I want the answer to be a list like ['AI,'AI'] etc.. so I can copy to python code. The examples to classify are the following!\n",
    "zero_shot_openai_reason = ['AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI',\n",
    " 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'Human', 'AI',\n",
    " 'AI', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI']\n",
    "\n",
    "# r1 model\n",
    "zero_shot_deepseek = ['AI', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'AI', 'Human', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI']\n",
    "# R1 explanation\n",
    "# Step-by-Step Explanation:\n",
    "\n",
    "# Structure & Flow: AI texts are more systematically structured with clear topic sentences, while human texts may have abrupt transitions or varied sentence lengths.\n",
    "\n",
    "# Technical Depth vs. Narrative: AI often balances detail with clarity; human texts might include niche references or less polished explanations.\n",
    "\n",
    "# Phrasing & Jargon: AI uses precise terminology consistently, while human writing might have colloquial phrases or minor inconsistencies.\n",
    "\n",
    "# Citations & Examples: Human texts may reference specific studies or historical anecdotes, whereas AI summarizes concepts broadly.\n",
    "\n",
    "# Errors & Quirks: Incomplete sentences (D1-23) or informal tones (D1-19) suggest human origin, while flawless, neutral tones indicate AI.\n",
    "\n",
    "# Each entry was evaluated based on these criteria to determine the most likely origin.\n",
    "\n",
    "# claude 3.7 Sonnet \n",
    "zero_shot_claude = ['Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading and eval helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np   \n",
    "df_input = pd.read_csv('../../datasets/val/dataset1_inputs.csv', sep='\\t')\n",
    "df_output = pd.read_csv('../../datasets/val/dataset1_outputs.csv', sep='\\t')\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def test_model(y_pred):\n",
    "    y_test = df_output['Label']\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AI       0.60      1.00      0.75        15\n",
      "       Human       1.00      0.33      0.50        15\n",
      "\n",
      "    accuracy                           0.67        30\n",
      "   macro avg       0.80      0.67      0.62        30\n",
      "weighted avg       0.80      0.67      0.62        30\n",
      "\n",
      "[[15  0]\n",
      " [10  5]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Não deu resposta a todos os inputs! Faltavam 3\n",
    "#test_model(zero_shot_openai) \n",
    "\n",
    "test_model(zero_shot_openai_reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AI       0.88      0.93      0.90        15\n",
      "       Human       0.93      0.87      0.90        15\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.90        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n",
      "[[14  1]\n",
      " [ 2 13]]\n"
     ]
    }
   ],
   "source": [
    "test_model(zero_shot_deepseek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AI       1.00      1.00      1.00        15\n",
      "       Human       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "[[15  0]\n",
      " [ 0 15]]\n"
     ]
    }
   ],
   "source": [
    "test_model(zero_shot_claude)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
