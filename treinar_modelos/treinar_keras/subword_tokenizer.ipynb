{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ġcat', 'Ġis', 'Ġcrying', 'Ġa', 'Ġlot', '!', 'ĠGoing', 'Ġchasing', '.', 'Ġ', 'ĠLast', 'ed', ',', 'Ġchased', ',', 'Ġsummar', 'ization']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pretrained BPE tokenizer \n",
    "#GPT-2\tBPE (Byte-level)\n",
    "#RoBERTa\tBPE\n",
    "#XLM-RoBERTa\tBPE\n",
    "#OpenAI CLIP\tBPE\n",
    "#T5\tSentencePiece BPE\n",
    "\n",
    "# https://huggingface.co/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer\n",
    "# Outros https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained\n",
    "\n",
    "\n",
    "#Constructs a RoBERTa tokenizer, derived from the GPT-2 tokenizer, using byte-level Byte-Pair-Encoding.\n",
    "#This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
    "#be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
    "\n",
    "# Para além disto também usa ponctuação como tokens\n",
    "tokenizer_bpe = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "# Example text\n",
    "text =  [\"The cat is crying a lot! Going chasing.  Lasted, chased, summarization\"] \n",
    "\n",
    "# \"summarization\" foi dividido em 2 'Ġsummar', 'ization'\n",
    "#  \"Lasted\" tambem 'ĠLast', 'ed'\n",
    "\n",
    "\n",
    "tokens = tokenizer_bpe.tokenize(text[0])\n",
    "ids = tokenizer_bpe.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#from nltk.tokenize import word_tokenize\n",
    "MAX_TOKENS = 10000\n",
    "\n",
    "tokenizer_bpe = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    text = text.numpy()  # Convert Tensor to NumPy array\n",
    "\n",
    "    # Check if it's a batch of strings or a single string\n",
    "    if isinstance(text, np.ndarray):  \n",
    "        text = [t.decode(\"utf-8\") if isinstance(t, bytes) else t for t in text]\n",
    "    elif isinstance(text, bytes):  \n",
    "        text = text.decode(\"utf-8\")\n",
    "\n",
    "    tokens = tokenizer_bpe.tokenize(text) # word_tokenize(text.numpy().decode(\"utf-8\"))  # Tokenize using NLTK\n",
    "    return  tf.ragged.constant(tokens)\n",
    "\n",
    "# Wrap function to make it TensorFlow-compatible\n",
    "def custom_split_fn(text):\n",
    "   # return tf.py_function(func=custom_tokenizer, inp=[text], Tout=tf.string)\n",
    "    return tf.py_function(func=custom_tokenizer, inp=[text], Tout=tf.RaggedTensor)\n",
    "\n",
    "# Create TextVectorization layer\n",
    "# text_vectorization_subgram = tf.keras.layers.TextVectorization(\n",
    "#     max_tokens=10000,\n",
    "#     output_mode=\"tf_idf\",\n",
    "#     standardize=None,  # Prevents default lowercasing and punctuation stripping\n",
    "#     split=custom_split_fn,  # Custom tokenizer\n",
    "# )\n",
    "# text_data = tf.constant([\"Hello world! NLP is fun.\", \"Let's build a custom tokenizer.\"])\n",
    "# text_vectorization_subgram.adapt(text_data)  # Adapt to dataset\n",
    "\n",
    "\n",
    "MAX_TOKENS = 5000 # experimentar com outros valores, 5k 10k 20k\n",
    "NAME = \"subgram.keras\"\n",
    "\n",
    "text_vectorization_subgram = TextVectorization(\n",
    "    ngrams=1,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    # for int mode\n",
    "    output_sequence_length=512,  # Truncate long sequences, transformers usually have 512 limit sequence\n",
    "    \n",
    "    output_mode=\"int\",\n",
    "    standardize=None,  # Prevents default lowercasing and punctuation stripping\n",
    "    split=custom_split_fn  # Custom tokenizer\n",
    ")\n",
    "\n",
    "RESULTS[NAME] = tabular_pipeline(NAME,text_vectorization_subgram,512)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
