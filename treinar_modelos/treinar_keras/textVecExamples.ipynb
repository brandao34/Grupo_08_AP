{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ggomes/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utility import load_dataset,tfidf_preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "# necessário download disto uma vez para o subword tokenizer\n",
    "#nltk.download('punkt_tab')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possíveis tipos de tratamento de input\n",
    "\n",
    "# Preprocessamento\n",
    "• Standardization – removing punctuation; upper case to lower case;\n",
    "stemming; etc\n",
    "• Tokenization - splitting the text into units (tokens), that may be words,\n",
    "groups of words, characters, groups of characters\n",
    "• Indexing the tokens – converting into a numerical vector (e.g. one-hot\n",
    "encoding)\n",
    "\n",
    "# DNN -> tabelas\n",
    "    1) Técnicas clássicas\n",
    "        word-level tokenization - cada palavra um token\n",
    "        subword tokenization - \"staring\" = \"star\" + \"ing\"\n",
    "        N-gram tokenization- habitual bigram, \"he was\"\n",
    "    2) Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, validation = load_dataset(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.06760473, 0.13520947, 0.06760473, 0.        ,\n",
       "        0.        , 0.1924053 , 0.06760473, 0.2028142 , 0.06760473,\n",
       "        0.06760473, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.06760473, 0.06760473, 0.        , 0.06760473, 0.        ,\n",
       "        0.06760473, 0.06760473, 0.        , 0.06760473, 0.06760473,\n",
       "        0.        , 0.        , 0.06760473, 0.06760473, 0.06760473,\n",
       "        0.09620265, 0.06760473, 0.06760473, 0.06760473, 0.06760473,\n",
       "        0.06760473, 0.        , 0.06760473, 0.06760473, 0.13520947,\n",
       "        0.06760473, 0.        , 0.06760473, 0.06760473, 0.        ,\n",
       "        0.        , 0.06760473, 0.13520947, 0.04810132, 0.06760473,\n",
       "        0.06760473, 0.06760473, 0.06760473, 0.        , 0.06760473,\n",
       "        0.        , 0.        , 0.06760473, 0.06760473, 0.06760473,\n",
       "        0.06760473, 0.04810132, 0.        , 0.06760473, 0.14430397,\n",
       "        0.        , 0.06760473, 0.06760473, 0.13520947, 0.06760473,\n",
       "        0.        , 0.06760473, 0.06760473, 0.06760473, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.06760473, 0.        , 0.04810132, 0.06760473, 0.04810132,\n",
       "        0.13520947, 0.        , 0.06760473, 0.06760473, 0.        ,\n",
       "        0.        , 0.2028142 , 0.06760473, 0.04810132, 0.09620265,\n",
       "        0.        , 0.06760473, 0.3848106 , 0.13520947, 0.        ,\n",
       "        0.        , 0.06760473, 0.06760473, 0.06760473, 0.13520947,\n",
       "        0.06760473, 0.06760473, 0.06760473, 0.        , 0.        ,\n",
       "        0.06760473, 0.        , 0.06760473, 0.        , 0.        ,\n",
       "        0.06760473, 0.06760473, 0.06760473, 0.        , 0.06760473,\n",
       "        0.        , 0.06760473, 0.13520947, 0.        , 0.06760473,\n",
       "        0.13520947, 0.06760473, 0.06760473, 0.06760473, 0.        ,\n",
       "        0.2028142 , 0.        , 0.06760473, 0.06760473, 0.13520947,\n",
       "        0.13520947, 0.        , 0.06760473, 0.        , 0.06760473,\n",
       "        0.        , 0.06760473, 0.        , 0.06760473, 0.        ,\n",
       "        0.06760473, 0.        , 0.06760473, 0.06760473, 0.14430397,\n",
       "        0.        , 0.09620265, 0.06760473, 0.06760473, 0.09620265,\n",
       "        0.09620265, 0.06760473, 0.04810132, 0.06760473, 0.06760473,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.09620265, 0.        , 0.06760473, 0.        ,\n",
       "        0.06760473, 0.        , 0.06760473, 0.06760473],\n",
       "       [0.12261946, 0.        , 0.        , 0.        , 0.06130973,\n",
       "        0.06130973, 0.21811188, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.06130973, 0.06130973, 0.06130973, 0.06130973,\n",
       "        0.        , 0.        , 0.30654864, 0.        , 0.24523891,\n",
       "        0.        , 0.        , 0.18392918, 0.        , 0.        ,\n",
       "        0.06130973, 0.06130973, 0.        , 0.        , 0.        ,\n",
       "        0.30535663, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.06130973, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.06130973, 0.        , 0.        , 0.06130973,\n",
       "        0.06130973, 0.        , 0.        , 0.04362238, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.06130973, 0.        ,\n",
       "        0.06130973, 0.06130973, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.08724475, 0.12261946, 0.        , 0.04362238,\n",
       "        0.06130973, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.06130973, 0.        , 0.        , 0.        , 0.06130973,\n",
       "        0.06130973, 0.06130973, 0.06130973, 0.06130973, 0.06130973,\n",
       "        0.        , 0.06130973, 0.08724475, 0.        , 0.04362238,\n",
       "        0.        , 0.06130973, 0.        , 0.        , 0.06130973,\n",
       "        0.42916809, 0.        , 0.        , 0.08724475, 0.17448951,\n",
       "        0.06130973, 0.        , 0.08724475, 0.        , 0.12261946,\n",
       "        0.06130973, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.06130973, 0.06130973,\n",
       "        0.        , 0.06130973, 0.        , 0.18392918, 0.06130973,\n",
       "        0.        , 0.        , 0.        , 0.12261946, 0.        ,\n",
       "        0.06130973, 0.        , 0.        , 0.06130973, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.06130973,\n",
       "        0.        , 0.06130973, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.06130973, 0.        , 0.12261946, 0.        ,\n",
       "        0.06130973, 0.        , 0.06130973, 0.        , 0.06130973,\n",
       "        0.        , 0.12261946, 0.        , 0.        , 0.04362238,\n",
       "        0.06130973, 0.04362238, 0.        , 0.        , 0.08724475,\n",
       "        0.30535663, 0.        , 0.04362238, 0.        , 0.        ,\n",
       "        0.06130973, 0.06130973, 0.06130973, 0.06130973, 0.06130973,\n",
       "        0.06130973, 0.04362238, 0.06130973, 0.        , 0.06130973,\n",
       "        0.        , 0.06130973, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "e1 = train.head(2)\n",
    "\n",
    "tfidf.fit_transform(e1['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.0519652 , 0.0519652 , 0.1039304 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14789467, 0.0519652 ,\n",
       "        0.0519652 , 0.        , 0.        , 0.0519652 , 0.0519652 ,\n",
       "        0.        , 0.0519652 , 0.0519652 , 0.1558956 , 0.0519652 ,\n",
       "        0.1039304 , 0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.        , 0.        , 0.        ,\n",
       "        0.0519652 , 0.0519652 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.        , 0.        , 0.        , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.07394734, 0.0519652 , 0.        ,\n",
       "        0.0519652 , 0.        , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.        , 0.        , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.1039304 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.        , 0.        ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.0519652 , 0.0519652 ,\n",
       "        0.1039304 , 0.1039304 , 0.03697367, 0.        , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.        , 0.        ,\n",
       "        0.0519652 , 0.0519652 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 , 0.03697367,\n",
       "        0.0519652 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.0519652 , 0.0519652 , 0.11092101, 0.        ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.        , 0.        ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 , 0.1039304 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 , 0.        ,\n",
       "        0.        , 0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.0519652 ,\n",
       "        0.0519652 , 0.        , 0.        , 0.03697367, 0.        ,\n",
       "        0.0519652 , 0.        , 0.0519652 , 0.0519652 , 0.03697367,\n",
       "        0.        , 0.0519652 , 0.1039304 , 0.0519652 , 0.0519652 ,\n",
       "        0.        , 0.        , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.1558956 , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.03697367, 0.0519652 , 0.        ,\n",
       "        0.        , 0.07394734, 0.03697367, 0.        , 0.0519652 ,\n",
       "        0.        , 0.        , 0.0519652 , 0.0519652 , 0.29578935,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.        , 0.0519652 ,\n",
       "        0.0519652 , 0.        , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.1039304 , 0.0519652 , 0.0519652 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 , 0.1039304 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.0519652 , 0.0519652 , 0.        ,\n",
       "        0.        , 0.0519652 , 0.0519652 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.        , 0.        , 0.0519652 , 0.0519652 , 0.        ,\n",
       "        0.        , 0.0519652 , 0.0519652 , 0.1039304 , 0.0519652 ,\n",
       "        0.0519652 , 0.        , 0.        , 0.0519652 , 0.0519652 ,\n",
       "        0.1039304 , 0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 , 0.        ,\n",
       "        0.        , 0.1558956 , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.        , 0.        , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.1039304 , 0.0519652 , 0.0519652 , 0.1039304 ,\n",
       "        0.0519652 , 0.        , 0.        , 0.0519652 , 0.0519652 ,\n",
       "        0.        , 0.        , 0.        , 0.0519652 , 0.0519652 ,\n",
       "        0.        , 0.        , 0.0519652 , 0.0519652 , 0.        ,\n",
       "        0.        , 0.0519652 , 0.0519652 , 0.        , 0.        ,\n",
       "        0.0519652 , 0.0519652 , 0.        , 0.        , 0.        ,\n",
       "        0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 , 0.11092101,\n",
       "        0.0519652 , 0.03697367, 0.0519652 , 0.        , 0.07394734,\n",
       "        0.0519652 , 0.        , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.0519652 , 0.07394734, 0.        , 0.0519652 ,\n",
       "        0.        , 0.0519652 , 0.07394734, 0.0519652 , 0.0519652 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.0519652 , 0.0519652 , 0.03697367,\n",
       "        0.0519652 , 0.        , 0.0519652 , 0.0519652 , 0.0519652 ,\n",
       "        0.0519652 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.07394734, 0.0519652 ,\n",
       "        0.03697367, 0.        , 0.        , 0.0519652 , 0.0519652 ,\n",
       "        0.        , 0.        , 0.0519652 , 0.0519652 , 0.        ,\n",
       "        0.        , 0.0519652 , 0.0519652 , 0.0519652 , 0.0519652 ],\n",
       "       [0.09533825, 0.09533825, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.04766912,\n",
       "        0.04766912, 0.04766912, 0.04766912, 0.16958487, 0.        ,\n",
       "        0.        , 0.09533825, 0.04766912, 0.        , 0.        ,\n",
       "        0.09533825, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04766912, 0.04766912, 0.04766912, 0.04766912, 0.04766912,\n",
       "        0.04766912, 0.04766912, 0.04766912, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.23834562, 0.09533825, 0.14300737,\n",
       "        0.        , 0.        , 0.19067649, 0.09533825, 0.04766912,\n",
       "        0.04766912, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.14300737, 0.09533825, 0.04766912, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.04766912, 0.04766912, 0.04766912,\n",
       "        0.04766912, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.23741882, 0.        , 0.28601474,\n",
       "        0.        , 0.04766912, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.04766912, 0.04766912, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.04766912, 0.04766912,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.04766912,\n",
       "        0.04766912, 0.04766912, 0.04766912, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.03391697, 0.04766912, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.04766912, 0.04766912,\n",
       "        0.        , 0.        , 0.04766912, 0.04766912, 0.04766912,\n",
       "        0.04766912, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.06783395,\n",
       "        0.        , 0.04766912, 0.04766912, 0.09533825, 0.04766912,\n",
       "        0.04766912, 0.        , 0.        , 0.03391697, 0.04766912,\n",
       "        0.        , 0.        , 0.        , 0.04766912, 0.04766912,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.04766912,\n",
       "        0.04766912, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.04766912, 0.04766912, 0.04766912,\n",
       "        0.04766912, 0.04766912, 0.04766912, 0.04766912, 0.04766912,\n",
       "        0.04766912, 0.04766912, 0.04766912, 0.04766912, 0.        ,\n",
       "        0.        , 0.04766912, 0.04766912, 0.06783395, 0.04766912,\n",
       "        0.        , 0.04766912, 0.        , 0.        , 0.03391697,\n",
       "        0.04766912, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04766912, 0.04766912, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04766912, 0.04766912, 0.33368386, 0.04766912,\n",
       "        0.04766912, 0.04766912, 0.04766912, 0.04766912, 0.04766912,\n",
       "        0.04766912, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.06783395, 0.        , 0.04766912,\n",
       "        0.04766912, 0.13566789, 0.06783395, 0.09533825, 0.        ,\n",
       "        0.04766912, 0.04766912, 0.        , 0.        , 0.06783395,\n",
       "        0.        , 0.        , 0.        , 0.04766912, 0.        ,\n",
       "        0.        , 0.04766912, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09533825, 0.04766912,\n",
       "        0.04766912, 0.04766912, 0.04766912, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.04766912, 0.04766912,\n",
       "        0.04766912, 0.04766912, 0.        , 0.        , 0.04766912,\n",
       "        0.04766912, 0.        , 0.        , 0.14300737, 0.04766912,\n",
       "        0.04766912, 0.04766912, 0.04766912, 0.04766912, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09533825, 0.09533825, 0.        , 0.        , 0.04766912,\n",
       "        0.04766912, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04766912, 0.04766912, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.04766912,\n",
       "        0.04766912, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04766912, 0.04766912, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04766912, 0.04766912, 0.        , 0.        ,\n",
       "        0.09533825, 0.04766912, 0.04766912, 0.        , 0.        ,\n",
       "        0.04766912, 0.04766912, 0.        , 0.        , 0.04766912,\n",
       "        0.04766912, 0.        , 0.        , 0.04766912, 0.04766912,\n",
       "        0.        , 0.        , 0.09533825, 0.04766912, 0.04766912,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.03391697,\n",
       "        0.        , 0.03391697, 0.        , 0.04766912, 0.03391697,\n",
       "        0.        , 0.04766912, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.06783395, 0.04766912, 0.        ,\n",
       "        0.04766912, 0.        , 0.23741882, 0.        , 0.        ,\n",
       "        0.04766912, 0.04766912, 0.04766912, 0.04766912, 0.04766912,\n",
       "        0.04766912, 0.04766912, 0.        , 0.        , 0.03391697,\n",
       "        0.        , 0.04766912, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04766912, 0.04766912, 0.04766912, 0.04766912,\n",
       "        0.04766912, 0.04766912, 0.04766912, 0.04766912, 0.04766912,\n",
       "        0.04766912, 0.04766912, 0.04766912, 0.03391697, 0.        ,\n",
       "        0.03391697, 0.04766912, 0.04766912, 0.        , 0.        ,\n",
       "        0.04766912, 0.04766912, 0.        , 0.        , 0.04766912,\n",
       "        0.04766912, 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english',ngram_range=(1,2))\n",
    "# \n",
    "\n",
    "\n",
    "e1 = train.head(2)\n",
    "\n",
    "tfidf.fit_transform(e1['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Coupling losses were studied in composite tape...\n",
      "1    In this study, we investigate the coupling los...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['2212', '2212 ag', '77', '77 tapes', 'account',\n",
       "       'account demagnetizing', 'account properties', 'actually',\n",
       "       'actually observed', 'advantages', 'advantages traditional',\n",
       "       'affects', 'affects coupling', 'ag', 'ag ag', 'ag alloy',\n",
       "       'ag interface', 'ag matrix', 'ag mg', 'ag poor', 'ag tapes',\n",
       "       'agree', 'agree nicely', 'alloy', 'alloy kind', 'alloy matrix',\n",
       "       'amplitude', 'amplitude frequency', 'amplitudes',\n",
       "       'amplitudes losses', 'annealed', 'annealed tapes', 'annealing',\n",
       "       'annealing tapes', 'applications', 'applications high', 'applied',\n",
       "       'applied magnetic', 'applying', 'applying model', 'architecture',\n",
       "       'architecture contrary', 'bi', 'bi 2212', 'bi columnar', 'bridges',\n",
       "       'bridges filaments', 'bscco', 'bscco ag', 'bscco tape',\n",
       "       'bscco tapes', 'case', 'case soft', 'chi_0', 'chi_0 takes',\n",
       "       'columnar', 'columnar bscco', 'columnar structure', 'composite',\n",
       "       'composite tapes', 'composition', 'composition length',\n",
       "       'configurations', 'configurations including', 'confirm',\n",
       "       'confirm validity', 'contact', 'contact interface', 'containing',\n",
       "       'containing superconducting', 'contrary', 'contrary sample',\n",
       "       'coupling', 'coupling currents', 'coupling loss',\n",
       "       'coupling losses', 'coupling superconducting', 'creation',\n",
       "       'creation intergrowths', 'currents', 'currents particular',\n",
       "       'cutting', 'cutting order', 'data', 'data low', 'demagnetizing',\n",
       "       'demagnetizing effects', 'demonstrated',\n",
       "       'demonstrated measurements', 'densely', 'densely packed',\n",
       "       'dependencies', 'dependencies agree', 'determined',\n",
       "       'determined explain', 'determined tape', 'developed',\n",
       "       'developed round', 'development', 'development practical',\n",
       "       'different', 'different matrix', 'discrepancies',\n",
       "       'discrepancies taking', 'disturbances', 'disturbances effects',\n",
       "       'dominated', 'dominated bi', 'dynamics', 'dynamics magnetic',\n",
       "       'effective', 'effective resistivity', 'effects', 'effects bi',\n",
       "       'effects established', 'electrical', 'electrical contact',\n",
       "       'electromagnetic', 'electromagnetic losses', 'embedded',\n",
       "       'embedded metallic', 'established', 'established susceptibility',\n",
       "       'exhibit', 'exhibit lower', 'expected', 'expected determined',\n",
       "       'experimental', 'experimental results', 'experiments',\n",
       "       'experiments coupling', 'explain', 'explain discrepancies',\n",
       "       'extensive', 'extensive creation', 'factor', 'factor chi_0',\n",
       "       'favorable', 'favorable studying', 'field', 'field amplitude',\n",
       "       'field disturbances', 'field specifically', 'fields',\n",
       "       'fields higher', 'fields observe', 'filamentary',\n",
       "       'filamentary architecture', 'filaments', 'filaments bscco',\n",
       "       'filaments embedded', 'filaments normal', 'filaments using',\n",
       "       'finally', 'finally use', 'flux', 'flux penetration', 'form',\n",
       "       'form separate', 'frequency', 'frequency dependencies',\n",
       "       'frequency measured', 'function', 'function magnetic',\n",
       "       'furthermore', 'furthermore coupling', 'geometrical',\n",
       "       'geometrical factor', 'geometry', 'geometry quite', 'given',\n",
       "       'given metal', 'high', 'high temperature', 'higher',\n",
       "       'higher plane', 'highly', 'highly relevant', 'important',\n",
       "       'important insights', 'including', 'including increased',\n",
       "       'increased', 'increased tolerance', 'influence',\n",
       "       'influence dynamics', 'insights', 'insights coupling', 'interface',\n",
       "       'interface coupling', 'interface superconducting',\n",
       "       'interface varies', 'intergrowths', 'intergrowths actually',\n",
       "       'investigate', 'investigate coupling', 'investigate influence',\n",
       "       'kind', 'kind sample', 'kind samples', 'known', 'known offer',\n",
       "       'larger', 'larger predicted', 'length', 'length samples',\n",
       "       'longitudinal', 'longitudinal matrix', 'loss', 'loss annealed',\n",
       "       'loss bi', 'loss dominated', 'loss lower', 'loss mechanisms',\n",
       "       'loss sensitive', 'loss values', 'losses', 'losses function',\n",
       "       'losses studied', 'losses vs', 'low', 'low amplitudes', 'lower',\n",
       "       'lower expected', 'lower loss', 'lower plane', 'magnetic',\n",
       "       'magnetic field', 'magnetic fields', 'magnetic flux', 'magnitude',\n",
       "       'magnitude applied', 'material', 'material form', 'matrix',\n",
       "       'matrix ag', 'matrix case', 'matrix composition',\n",
       "       'matrix demonstrated', 'matrix determined', 'matrix effective',\n",
       "       'matrix orientation', 'matrix provoke', 'matrix pure',\n",
       "       'matrix value', 'measured', 'measured quantities',\n",
       "       'measured temperature', 'measurements', 'measurements bi',\n",
       "       'measurements tapes', 'mechanisms', 'mechanisms bi', 'metal',\n",
       "       'metal resistivity', 'metallic', 'metallic matrix', 'mg',\n",
       "       'mg alloy', 'model', 'model developed', 'model effective',\n",
       "       'multifilamentary', 'multifilamentary wires', 'nicely',\n",
       "       'nicely theoretical', 'normal', 'normal matrix', 'numerical',\n",
       "       'numerical simulations', 'observe', 'observe annealing',\n",
       "       'observed', 'observed kind', 'offer', 'offer advantages', 'order',\n",
       "       'order investigate', 'orientation', 'orientation ag',\n",
       "       'orientation finally', 'orientation magnitude', 'overall',\n",
       "       'overall study', 'packed', 'packed filaments', 'particular',\n",
       "       'particular role', 'penetration', 'penetration geometrical',\n",
       "       'plane', 'plane magnetic', 'poor', 'poor quality', 'practical',\n",
       "       'practical applications', 'predicted', 'predicted theory',\n",
       "       'properties', 'properties alloy', 'properties electrical',\n",
       "       'provides', 'provides important', 'provoke', 'provoke extensive',\n",
       "       'pure', 'pure ag', 'pure silver', 'quality', 'quality properties',\n",
       "       'quantities', 'quantities tape', 'quite', 'quite favorable',\n",
       "       'relevant', 'relevant development', 'resistivity',\n",
       "       'resistivity filamentary', 'resistivity lower',\n",
       "       'resistivity matrix', 'results', 'results overall', 'role',\n",
       "       'role superconducting', 'round', 'round multifilamentary',\n",
       "       'sample', 'sample ag', 'sample geometry', 'samples',\n",
       "       'samples varied', 'sensitive', 'sensitive orientation', 'separate',\n",
       "       'separate stacks', 'significantly', 'significantly affects',\n",
       "       'significantly orientation', 'silver', 'silver matrix',\n",
       "       'simulations', 'simulations confirm', 'soft', 'soft matrix',\n",
       "       'specifically', 'specifically coupling', 'stacks',\n",
       "       'stacks densely', 'structure', 'structure bscco', 'studied',\n",
       "       'studied composite', 'study', 'study investigate',\n",
       "       'study provides', 'studying', 'studying coupling', 'subsequent',\n",
       "       'subsequent cutting', 'superconducting', 'superconducting bridges',\n",
       "       'superconducting filaments', 'superconducting material',\n",
       "       'superconductors', 'susceptibility', 'susceptibility data',\n",
       "       'susceptibility measurements', 'susceptibility technique', 'takes',\n",
       "       'takes account', 'taking', 'taking account', 'tape',\n",
       "       'tape configurations', 'tape pure', 'tape understood',\n",
       "       'tape using', 'tapes', 'tapes containing', 'tapes different',\n",
       "       'tapes exhibit', 'tapes furthermore', 'tapes highly',\n",
       "       'tapes known', 'tapes significantly', 'tapes transverse',\n",
       "       'tapes using', 'technique', 'technique electromagnetic',\n",
       "       'temperature', 'temperature 77', 'temperature superconductors',\n",
       "       'theoretical', 'theoretical model', 'theory', 'theory given',\n",
       "       'tolerance', 'tolerance magnetic', 'traditional',\n",
       "       'traditional tape', 'transverse', 'transverse longitudinal',\n",
       "       'unannealed', 'unannealed tapes', 'understood',\n",
       "       'understood experiments', 'use', 'use numerical', 'using',\n",
       "       'using measured', 'using susceptibility', 'validity',\n",
       "       'validity experimental', 'value', 'value larger', 'values',\n",
       "       'values unannealed', 'varied', 'varied subsequent', 'varies',\n",
       "       'varies significantly', 'vs', 'vs frequency', 'wires',\n",
       "       'wires applying'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(e1['text'])\n",
    "\n",
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coupling losses were studied in composite tapes containing superconducting material in the form of two separate stacks of densely packed filaments embedded in a metallic matrix of Ag or Ag alloy. This kind of sample geometry is quite favorable for studying the coupling currents and in particular the role of superconducting bridges between filaments. By using a.c. susceptibility technique, the electromagnetic losses as function of a.c. magnetic field amplitude and frequency were measured at the temperature T = 77 K for two tapes with different matrix composition. The length of samples was varied by subsequent cutting in order to investigate its influence on the dynamics of magnetic flux penetration. The geometrical factor $\\\\chi_0$ which takes into account the demagnetizing effects was established from a.c. susceptibility data at low amplitudes. Losses vs frequency dependencies have been found to agree nicely with the theoretical model developed for round multifilamentary wires.\\n\\nApplying this model, the effective resistivity of the matrix was determined for each tape, by using only measured quantities. For the tape with pure silver matrix its value was found to be larger than what predicted by the theory for given metal resistivity and filamentary architecture. On the contrary, in the sample with a Ag/Mg alloy matrix, an effective resistivity much lower than expected was determined. We explain these discrepancies by taking into account the properties of the electrical contact of the interface between the superconducting filaments and the normal matrix. In the case of soft matrix of pure Ag, this is of poor quality, while the properties of alloy matrix seem to provoke an extensive creation of intergrowths which can be actually observed in this kind of samples.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/ggomes/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!' '$' 'a' 'be' 'cat' 'chase' 'cry' 'deep-learning' 'go' 'hbar' 'hello'\n",
      " 'ing' 'lot' 'machine_learning' 'ng' 'ola' 'the' 'world' '{']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ggomes/miniconda3/envs/tf-wsl/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# trash_tokens= ['$','/','\\\\','|','^','{','}','[',']',';','_']\n",
    "# #\n",
    "# trash_tokens_pattern = r\"|\".join(trash_tokens)\n",
    "# def custom_tokenizer(text):\n",
    "# #    tokens = re.findall(r'\\b\\w+\\b', text)  # Extract words\n",
    "#     tokens = re.findall(r'\\b.+\\b', text)  # Extract words\n",
    "#     return [t for t in tokens]  # Filter out unwanted tokens\n",
    "\n",
    "#     #return [t for t in tokens if re.search(trash_tokens_pattern, t)]  # Filter out unwanted tokens\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatized_subword_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    subwords = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, pos='v')  # Convert verb forms to base form\n",
    "        if lemma != token:\n",
    "            subwords.append(lemma)\n",
    "            subwords.append(token[len(lemma):])  # Extract suffix\n",
    "        else:\n",
    "            subwords.append(token)\n",
    "    \n",
    "    return [s for s in subwords if s]  # Remove empty strings\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmatized_subword_tokenizer, lowercase=True)\n",
    "\n",
    "\n",
    "\n",
    "# vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, lowercase=True)\n",
    "text_data = [\"The cat is crying a lot! Going chasing\", \"$$hello world\", \"machine_learning\", \"ola\", \"deep-learning\",\"{hbar\"]\n",
    "\n",
    "X = vectorizer.fit_transform(text_data) \n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ġcat', 'Ġis', 'Ġcrying', 'Ġa', 'Ġlot', '!', 'ĠGoing', 'Ġchasing', '.', 'Ġ', 'ĠLast', 'ed', ',', 'Ġchased', ',', 'Ġsummar', 'ization']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pretrained BPE tokenizer \n",
    "#GPT-2\tBPE (Byte-level)\n",
    "#RoBERTa\tBPE\n",
    "#XLM-RoBERTa\tBPE\n",
    "#OpenAI CLIP\tBPE\n",
    "#T5\tSentencePiece BPE\n",
    "\n",
    "# https://huggingface.co/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer\n",
    "# Outros https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained\n",
    "\n",
    "\n",
    "#Constructs a RoBERTa tokenizer, derived from the GPT-2 tokenizer, using byte-level Byte-Pair-Encoding.\n",
    "#This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
    "#be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
    "\n",
    "# Para além disto também usa ponctuação como tokens\n",
    "tokenizer_bpe = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "# Example text\n",
    "text =  [\"The cat is crying a lot! Going chasing.  Lasted, chased, summarization\"] \n",
    "\n",
    "# \"summarization\" foi dividido em 2 'Ġsummar', 'ization'\n",
    "#  \"Lasted\" tambem 'ĠLast', 'ed'\n",
    "\n",
    "\n",
    "tokens = tokenizer_bpe.tokenize(text[0])\n",
    "ids = tokenizer_bpe.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$/\\\\\\\\|^{}[]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trash_tokens_pattern = r\"$/\\\\|^{}[]\"\n",
    "trash_tokens_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nXLNetTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XLNetTokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m XLNetTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlnet/xlnet-base-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe cat is crying a lot! Going chasing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-wsl/lib/python3.12/site-packages/transformers/utils/import_utils.py:1736\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1736\u001b[0m requires_backends(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_backends)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-wsl/lib/python3.12/site-packages/transformers/utils/import_utils.py:1724\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1722\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1724\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nXLNetTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# from transformers import XLNetTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "#tokenizer = XLNetTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")\n",
    "\n",
    "tokenizer.tokenize(\"The cat is crying a lot! Going chasing\")\n",
    "# [\"i\", \"have\", \"a\", \"new\", \"gp\", \"##u\", \"!\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
