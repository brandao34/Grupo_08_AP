{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline de Treinamento RNN para Detecção de Texto\n",
    "## (AI vs Human) - Multi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from rnn import RNN  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para Carregar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_version):\n",
    "    base_path = f'../../datasets/{dataset_version}/'\n",
    "    train_file = f'Dataset{dataset_version}_train_clean.csv'\n",
    "    test_file = f'Dataset{dataset_version}_test_clean.csv'\n",
    "    validation_file = f'Dataset{dataset_version}_validation_clean.csv'\n",
    "    \n",
    "    train = pd.read_csv(os.path.join(base_path, train_file), sep=',')\n",
    "    test = pd.read_csv(os.path.join(base_path, test_file), sep=',')\n",
    "    \n",
    "    # Verifica se o arquivo de validação existe\n",
    "    validation_path = os.path.join(base_path, validation_file)\n",
    "    if os.path.exists(validation_path):\n",
    "        validation = pd.read_csv(validation_path, sep=',')\n",
    "    else:\n",
    "        validation = None  # Define como None se não existir\n",
    "    \n",
    "    return train, test, validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento com UNIgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"unigram_be.keras\"\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Configuração do TextVectorization para unigramas\n",
    "text_vectorization_singlegram_be = TextVectorization(\n",
    "    max_tokens=900,  # Número máximo de tokens\n",
    "    output_mode=\"multi_hot\",  # Representação multi-hot\n",
    "    standardize=\"lower_and_strip_punctuation\",  # Normalização do texto\n",
    ")\n",
    "def preprocess_text(text_ds):\n",
    "    text_ds_2 = text_ds['text']\n",
    "    text_vectorization_singlegram_be.adapt(text_ds_2)\n",
    "    vectorized_text = text_ds_2.map(lambda x: text_vectorization_singlegram_be(x))\n",
    "    return np.array(list(vectorized_text)) \n",
    "\n",
    "def unigram_preprocessing(train, test, validation):\n",
    "    # Pré-processa apenas a coluna de texto\n",
    "    X_train = preprocess_text(train)\n",
    "    X_test = preprocess_text(test)\n",
    "    \n",
    "    # Converte os rótulos para valores numéricos\n",
    "    y_train = train['Label'].map({'AI': 1, 'Human': 0}).values\n",
    "    y_test = test['Label'].map({'AI': 1, 'Human': 0}).values\n",
    "    \n",
    "    # Processa validação apenas se existir\n",
    "    X_val = None\n",
    "    y_val = None\n",
    "    if validation is not None:\n",
    "        X_val = preprocess_text(validation)\n",
    "        y_val = validation['Label'].map({'AI': 1, 'Human': 0}).values\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Treinamento da RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_rnn(X_train, y_train, X_test, y_test, X_val, y_val, input_size):\n",
    "    # Reshape para formato sequencial\n",
    "    X_train_seq = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_test_seq = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "    \n",
    "    print(\"\\nIniciando treinamento da RNN...\")\n",
    "\n",
    "    rnn = RNN(\n",
    "        input_size=input_size,\n",
    "        hidden_size=64,\n",
    "        output_size=1,\n",
    "        lr=0.08,\n",
    "        dropout_rate=0.2\n",
    "    )\n",
    "    \n",
    "    rnn.train(X_train_seq, y_train, epochs=100)\n",
    "    \n",
    "    rnn_pred = rnn.predict(X_test_seq)\n",
    "    print(\"\\nRelatório de Classificação:\")\n",
    "    print(classification_report(y_test, rnn_pred))\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(confusion_matrix(y_test, rnn_pred))\n",
    "    \n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: (14000, 5000)\n",
      "\n",
      "Iniciando treinamento da RNN...\n",
      "Epoch 0: Loss = 0.6930487900649178, Accuracy = 50.50%\n",
      "Epoch 99: Loss = 0.3420467668577256, Accuracy = 95.27%\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.80      0.74      1500\n",
      "           1       0.76      0.63      0.69      1500\n",
      "\n",
      "    accuracy                           0.72      3000\n",
      "   macro avg       0.72      0.72      0.72      3000\n",
      "weighted avg       0.72      0.72      0.72      3000\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[1204  296]\n",
      " [ 549  951]]\n",
      "\n",
      "Pipeline concluído! Modelos salvos em 'modelos_rnn/'.\n",
      "     ID                                               Text\n",
      "0  D1-1  The cell cycle, or cell-division cycle, is the...\n",
      "1  D1-2  The cell cycle is the process by which a cell ...\n",
      "2  D1-3  Photons, in many atomic models in physics, are...\n",
      "3  D1-4  A photon is a fundamental particle of light an...\n",
      "4  D1-5  According to the theory of plate tectonics, Ea...\n",
      "\n",
      "Relatório Final:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AI       0.00      0.00      0.00        15\n",
      "       Human       0.48      0.93      0.64        15\n",
      "\n",
      "    accuracy                           0.47        30\n",
      "   macro avg       0.24      0.47      0.32        30\n",
      "weighted avg       0.24      0.47      0.32        30\n",
      "\n",
      "Matriz de Confusão Final:\n",
      "[[ 0 15]\n",
      " [ 1 14]]\n"
     ]
    }
   ],
   "source": [
    "# Configurações\n",
    "DATASET_VERSION = 3\n",
    "SAVE_DIR = 'modelos_rnn/'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Carregar dados\n",
    "train, test, validation = load_dataset(DATASET_VERSION)\n",
    "\n",
    "# Pré-processamento\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = unigram_preprocessing(train, test, validation)\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "# Treinar RNN\n",
    "rnn_model = train_evaluate_rnn(X_train, y_train, X_test, y_test, X_val, y_val, input_size=X_train.shape[1])\n",
    "\n",
    "# Salvar modelos\n",
    "with open(f'{SAVE_DIR}rnn_dataset{DATASET_VERSION}.pkl', 'wb') as f:\n",
    "    pickle.dump(rnn_model, f)\n",
    "\n",
    "\n",
    "print(\"\\nPipeline concluído! Modelos salvos em 'modelos_rnn/'.\")\n",
    "\n",
    "\n",
    "def preprocessing_inference(df, vectorizer):\n",
    "    text_data = df['Text']  \n",
    "    X = vectorizer(text_data).numpy()  \n",
    "    return X.reshape(X.shape[0], 1, X.shape[1])  # Ajusta para o formato sequencial\n",
    "\n",
    "# Certifique-se de que o TextVectorization foi adaptado\n",
    "text_vectorization_singlegram_be.adapt(train['text'])  \n",
    "\n",
    "# Carregar modelo\n",
    "with open(f'{SAVE_DIR}rnn_dataset{DATASET_VERSION}.pkl', 'rb') as f:\n",
    "    rnn_model = pickle.load(f)\n",
    "\n",
    "# Carregar dados\n",
    "df_input = pd.read_csv('../../datasets/val/dataset1_inputs.csv', sep='\\t')\n",
    "df_output = pd.read_csv('../../datasets/val/dataset1_outputs.csv', sep='\\t')\n",
    "\n",
    "# Previsões\n",
    "X_new = preprocessing_inference(df_input, text_vectorization_singlegram_be)\n",
    "predictions = rnn_model.predict(X_new)\n",
    "df_output['Predicted'] = np.where(predictions >= 0.5, 'AI', 'Human')\n",
    "\n",
    "# Métricas\n",
    "print(\"\\nRelatório Final:\")\n",
    "print(classification_report(df_output['Label'], df_output['Predicted']))\n",
    "print(\"Matriz de Confusão Final:\")\n",
    "print(confusion_matrix(df_output['Label'], df_output['Predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m train, test, validation = load_dataset(DATASET_VERSION)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Pré-processamento\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m X_train, y_train, X_test, y_test, X_val, y_val = \u001b[43munigram_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mForma de X_train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Treinar RNN\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36munigram_preprocessing\u001b[39m\u001b[34m(train, test, validation)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munigram_preprocessing\u001b[39m(train, test, validation):\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Pré-processa apenas a coluna de texto\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     X_train = \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     X_test = preprocess_text(test)\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Converte os rótulos para valores numéricos\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mpreprocess_text\u001b[39m\u001b[34m(text_ds)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_text\u001b[39m(text_ds):\n\u001b[32m     11\u001b[39m     text_ds_2 = text_ds[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mtext_vectorization_singlegram_be\u001b[49m\u001b[43m.\u001b[49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_ds_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     vectorized_text = text_ds_2.map(\u001b[38;5;28;01mlambda\u001b[39;00m x: text_vectorization_singlegram_be(x))\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array(\u001b[38;5;28mlist\u001b[39m(vectorized_text))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:428\u001b[39m, in \u001b[36mTextVectorization.adapt\u001b[39m\u001b[34m(self, data, batch_size, steps)\u001b[39m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data.shape.rank == \u001b[32m1\u001b[39m:\n\u001b[32m    425\u001b[39m         \u001b[38;5;66;03m# A plain list of strings\u001b[39;00m\n\u001b[32m    426\u001b[39m         \u001b[38;5;66;03m# is treated as as many documents\u001b[39;00m\n\u001b[32m    427\u001b[39m         data = tf.expand_dims(data, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_state()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:432\u001b[39m, in \u001b[36mTextVectorization.update_state\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28mself\u001b[39m._lookup_layer.update_state(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:553\u001b[39m, in \u001b[36mTextVectorization._preprocess\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    549\u001b[39m         inputs = tf.squeeze(inputs, axis=-\u001b[32m1\u001b[39m)\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._split == \u001b[33m\"\u001b[39m\u001b[33mwhitespace\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# This treats multiple whitespaces as one whitespace, and strips\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# leading and trailing whitespace.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     inputs = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._split == \u001b[33m\"\u001b[39m\u001b[33mcharacter\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    555\u001b[39m     inputs = tf.strings.unicode_split(inputs, \u001b[33m\"\u001b[39m\u001b[33mUTF-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1262\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/tensorflow/python/ops/ragged/ragged_string_ops.py:516\u001b[39m, in \u001b[36mstring_split_v2\u001b[39m\u001b[34m(input, sep, maxsplit, name)\u001b[39m\n\u001b[32m    514\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m string_split_v2(array_ops_stack.stack([\u001b[38;5;28minput\u001b[39m]), sep, maxsplit)[\u001b[32m0\u001b[39m]\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m rank == \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m   sparse_result = \u001b[43mstring_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstring_split_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m ragged_tensor.RaggedTensor.from_value_rowids(\n\u001b[32m    519\u001b[39m       values=sparse_result.values,\n\u001b[32m    520\u001b[39m       value_rowids=sparse_result.indices[:, \u001b[32m0\u001b[39m],\n\u001b[32m    521\u001b[39m       nrows=sparse_result.dense_shape[\u001b[32m0\u001b[39m],\n\u001b[32m    522\u001b[39m       validate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/tensorflow/python/ops/string_ops.py:283\u001b[39m, in \u001b[36mstring_split_v2\u001b[39m\u001b[34m(source, sep, maxsplit)\u001b[39m\n\u001b[32m    280\u001b[39m sep = ops.convert_to_tensor(sep, dtype=dtypes.string)\n\u001b[32m    281\u001b[39m source = ops.convert_to_tensor(source, dtype=dtypes.string)\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m indices, values, shape = \u001b[43mgen_string_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstring_split_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m indices.set_shape([\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m2\u001b[39m])\n\u001b[32m    286\u001b[39m values.set_shape([\u001b[38;5;28;01mNone\u001b[39;00m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/tensorflow/python/ops/gen_string_ops.py:1379\u001b[39m, in \u001b[36mstring_split_v2\u001b[39m\u001b[34m(input, sep, maxsplit, name)\u001b[39m\n\u001b[32m   1377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m   1378\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1379\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mStringSplitV2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxsplit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1381\u001b[39m     _result = _StringSplitV2Output._make(_result)\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Configurações\n",
    "\n",
    "DATASET_VERSION = 4\n",
    "SAVE_DIR = 'modelos_rnn/'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Carregar dados\n",
    "train, test, validation = load_dataset(DATASET_VERSION)\n",
    "\n",
    "# Pré-processamento\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = unigram_preprocessing(train, test, validation)\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "# Treinar RNN\n",
    "rnn_model = train_evaluate_rnn(X_train, y_train, X_test, y_test, X_val, y_val, input_size=X_train.shape[1])\n",
    "\n",
    "# Salvar modelos\n",
    "with open(f'{SAVE_DIR}rnn_dataset{DATASET_VERSION}.pkl', 'wb') as f:\n",
    "    pickle.dump(rnn_model, f)\n",
    "\n",
    "\n",
    "print(\"\\nPipeline concluído! Modelos salvos em 'modelos_rnn/'.\")\n",
    "\n",
    "\n",
    "def preprocessing_inference(df, vectorizer):\n",
    "    print(df.head())\n",
    "    text_data = df['Text']  \n",
    "    X = vectorizer(text_data).numpy()  \n",
    "    return X.reshape(X.shape[0], 1, X.shape[1])  # Ajusta para o formato sequencial\n",
    "\n",
    "# Certifique-se de que o TextVectorization foi adaptado\n",
    "text_vectorization_singlegram_be.adapt(train['text'])  \n",
    "\n",
    "# Carregar modelo\n",
    "with open(f'{SAVE_DIR}rnn_dataset{DATASET_VERSION}.pkl', 'rb') as f:\n",
    "    rnn_model = pickle.load(f)\n",
    "\n",
    "# Carregar dados\n",
    "df_input = pd.read_csv('../../datasets/val/dataset1_inputs.csv', sep='\\t')\n",
    "df_output = pd.read_csv('../../datasets/val/dataset1_outputs.csv', sep='\\t')\n",
    "\n",
    "# Previsões\n",
    "X_new = preprocessing_inference(df_input, text_vectorization_singlegram_be)\n",
    "predictions = rnn_model.predict(X_new)\n",
    "df_output['Predicted'] = np.where(predictions >= 0.5, 'AI', 'Human')\n",
    "\n",
    "# Métricas\n",
    "print(\"\\nRelatório Final:\")\n",
    "print(classification_report(df_output['Label'], df_output['Predicted']))\n",
    "print(\"Matriz de Confusão Final:\")\n",
    "print(confusion_matrix(df_output['Label'], df_output['Predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "When using `output_mode=multi_hot` and `pad_to_max_tokens=False`, the vocabulary size cannot be changed after the layer is called. Old vocab size is 5000, new vocab size is 925",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m train, test, validation = load_dataset(DATASET_VERSION)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Pré-processamento\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m X_train, y_train, X_test, y_test, X_val, y_val = \u001b[43munigram_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mForma de X_train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Treinar RNN\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36munigram_preprocessing\u001b[39m\u001b[34m(train, test, validation)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munigram_preprocessing\u001b[39m(train, test, validation):\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Pré-processa apenas a coluna de texto\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     X_train = \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     X_test = preprocess_text(test)\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Converte os rótulos para valores numéricos\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mpreprocess_text\u001b[39m\u001b[34m(text_ds)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_text\u001b[39m(text_ds):\n\u001b[32m     11\u001b[39m     text_ds_2 = text_ds[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mtext_vectorization_singlegram_be\u001b[49m\u001b[43m.\u001b[49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_ds_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     vectorized_text = text_ds_2.map(\u001b[38;5;28;01mlambda\u001b[39;00m x: text_vectorization_singlegram_be(x))\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array(\u001b[38;5;28mlist\u001b[39m(vectorized_text))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:429\u001b[39m, in \u001b[36mTextVectorization.adapt\u001b[39m\u001b[34m(self, data, batch_size, steps)\u001b[39m\n\u001b[32m    427\u001b[39m         data = tf.expand_dims(data, -\u001b[32m1\u001b[39m)\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mself\u001b[39m.update_state(data)\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfinalize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:435\u001b[39m, in \u001b[36mTextVectorization.finalize_state\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfinalize_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lookup_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/keras/src/layers/preprocessing/index_lookup.py:688\u001b[39m, in \u001b[36mIndexLookup.finalize_state\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    684\u001b[39m \u001b[38;5;66;03m# We call this here to save memory, now that we've built our vocabulary,\u001b[39;00m\n\u001b[32m    685\u001b[39m \u001b[38;5;66;03m# we don't want to keep every token we've seen in separate lookup\u001b[39;00m\n\u001b[32m    686\u001b[39m \u001b[38;5;66;03m# tables.\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m.reset_state()\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_record_vocabulary_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/keras/src/layers/preprocessing/index_lookup.py:348\u001b[39m, in \u001b[36mIndexLookup._record_vocabulary_size\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_record_vocabulary_size\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_vocab_size_unchanged\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tf.init_scope():\n\u001b[32m    350\u001b[39m         \u001b[38;5;28mself\u001b[39m._frozen_vocab_size = \u001b[38;5;28mself\u001b[39m.vocabulary_size()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/keras/src/layers/preprocessing/index_lookup.py:926\u001b[39m, in \u001b[36mIndexLookup._ensure_vocab_size_unchanged\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    920\u001b[39m     new_vocab_size = \u001b[38;5;28mself\u001b[39m.vocabulary_size()\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    923\u001b[39m     \u001b[38;5;28mself\u001b[39m._frozen_vocab_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m new_vocab_size != \u001b[38;5;28mself\u001b[39m._frozen_vocab_size\n\u001b[32m    925\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    927\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWhen using `output_mode=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.output_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    928\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand `pad_to_max_tokens=False`, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    929\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe vocabulary size cannot be changed after the layer is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    930\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcalled. Old vocab size is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._frozen_vocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    931\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnew vocab size is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_vocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    932\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: When using `output_mode=multi_hot` and `pad_to_max_tokens=False`, the vocabulary size cannot be changed after the layer is called. Old vocab size is 5000, new vocab size is 925"
     ]
    }
   ],
   "source": [
    "# Configurações\n",
    "DATASET_VERSION = 5\n",
    "SAVE_DIR = 'modelos_rnn/'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Carregar dados\n",
    "train, test, validation = load_dataset(DATASET_VERSION)\n",
    "\n",
    "# Pré-processamento\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = unigram_preprocessing(train, test, validation)\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "# Treinar RNN\n",
    "rnn_model = train_evaluate_rnn(X_train, y_train, X_test, y_test, X_val, y_val, input_size=X_train.shape[1])\n",
    "\n",
    "# Salvar modelos\n",
    "with open(f'{SAVE_DIR}rnn_dataset{DATASET_VERSION}.pkl', 'wb') as f:\n",
    "    pickle.dump(rnn_model, f)\n",
    "\n",
    "\n",
    "print(\"\\nPipeline concluído! Modelos salvos em 'modelos_rnn/'.\")\n",
    "\n",
    "\n",
    "def preprocessing_inference(df, vectorizer):\n",
    "    print(df.head())\n",
    "    text_data = df['Text']  \n",
    "    X = vectorizer(text_data).numpy()  \n",
    "    return X.reshape(X.shape[0], 1, X.shape[1])  # Ajusta para o formato sequencial\n",
    "\n",
    "# Certifique-se de que o TextVectorization foi adaptado\n",
    "text_vectorization_singlegram_be.adapt(train['text'])  \n",
    "\n",
    "# Carregar modelo\n",
    "with open(f'{SAVE_DIR}rnn_dataset{DATASET_VERSION}.pkl', 'rb') as f:\n",
    "    rnn_model = pickle.load(f)\n",
    "\n",
    "# Carregar dados\n",
    "df_input = pd.read_csv('../../datasets/val/dataset1_inputs.csv', sep='\\t')\n",
    "df_output = pd.read_csv('../../datasets/val/dataset1_outputs.csv', sep='\\t')\n",
    "\n",
    "# Previsões\n",
    "X_new = preprocessing_inference(df_input, text_vectorization_singlegram_be)\n",
    "predictions = rnn_model.predict(X_new)\n",
    "df_output['Predicted'] = np.where(predictions >= 0.5, 'AI', 'Human')\n",
    "\n",
    "# Métricas\n",
    "print(\"\\nRelatório Final:\")\n",
    "print(classification_report(df_output['Label'], df_output['Predicted']))\n",
    "print(\"Matriz de Confusão Final:\")\n",
    "print(confusion_matrix(df_output['Label'], df_output['Predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: (3245, 5000)\n",
      "\n",
      "Iniciando treinamento da RNN...\n",
      "Epoch 0: Loss = 0.6939806997847989, Accuracy = 39.69%\n",
      "Epoch 99: Loss = 0.13701189397312408, Accuracy = 99.35%\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85       421\n",
      "           1       0.92      0.71      0.80       391\n",
      "\n",
      "    accuracy                           0.83       812\n",
      "   macro avg       0.85      0.83      0.83       812\n",
      "weighted avg       0.85      0.83      0.83       812\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[396  25]\n",
      " [112 279]]\n",
      "\n",
      "Pipeline concluído! Modelos salvos em 'modelos_rnn/'.\n",
      "     ID                                               Text\n",
      "0  D1-1  The cell cycle, or cell-division cycle, is the...\n",
      "1  D1-2  The cell cycle is the process by which a cell ...\n",
      "2  D1-3  Photons, in many atomic models in physics, are...\n",
      "3  D1-4  A photon is a fundamental particle of light an...\n",
      "4  D1-5  According to the theory of plate tectonics, Ea...\n",
      "\n",
      "Relatório Final:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AI       0.00      0.00      0.00        15\n",
      "       Human       0.50      1.00      0.67        15\n",
      "\n",
      "    accuracy                           0.50        30\n",
      "   macro avg       0.25      0.50      0.33        30\n",
      "weighted avg       0.25      0.50      0.33        30\n",
      "\n",
      "Matriz de Confusão Final:\n",
      "[[ 0 15]\n",
      " [ 0 15]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cid34senhas/miniconda3/envs/DeepL/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/cid34senhas/miniconda3/envs/DeepL/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/cid34senhas/miniconda3/envs/DeepL/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Configurações\n",
    "DATASET_VERSION = 6\n",
    "SAVE_DIR = 'modelos_rnn/'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Carregar dados\n",
    "train, test, validation = load_dataset(DATASET_VERSION)\n",
    "\n",
    "# Pré-processamento\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = unigram_preprocessing(train, test, validation)\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "# Treinar RNN\n",
    "rnn_model = train_evaluate_rnn(X_train, y_train, X_test, y_test, X_val, y_val, input_size=X_train.shape[1])\n",
    "\n",
    "# Salvar modelos\n",
    "with open(f'{SAVE_DIR}rnn_dataset{DATASET_VERSION}.pkl', 'wb') as f:\n",
    "    pickle.dump(rnn_model, f)\n",
    "\n",
    "\n",
    "print(\"\\nPipeline concluído! Modelos salvos em 'modelos_rnn/'.\")\n",
    "\n",
    "\n",
    "def preprocessing_inference(df, vectorizer):\n",
    "    print(df.head())\n",
    "    text_data = df['Text']  \n",
    "    X = vectorizer(text_data).numpy()  \n",
    "    return X.reshape(X.shape[0], 1, X.shape[1])  # Ajusta para o formato sequencial\n",
    "\n",
    "# Certifique-se de que o TextVectorization foi adaptado\n",
    "text_vectorization_singlegram_be.adapt(train['text'])  \n",
    "\n",
    "# Carregar modelo\n",
    "with open(f'{SAVE_DIR}rnn_dataset{DATASET_VERSION}.pkl', 'rb') as f:\n",
    "    rnn_model = pickle.load(f)\n",
    "\n",
    "# Carregar dados\n",
    "df_input = pd.read_csv('../../datasets/val/dataset1_inputs.csv', sep='\\t')\n",
    "df_output = pd.read_csv('../../datasets/val/dataset1_outputs.csv', sep='\\t')\n",
    "\n",
    "# Previsões\n",
    "X_new = preprocessing_inference(df_input, text_vectorization_singlegram_be)\n",
    "predictions = rnn_model.predict(X_new)\n",
    "df_output['Predicted'] = np.where(predictions >= 0.5, 'AI', 'Human')\n",
    "\n",
    "# Métricas\n",
    "print(\"\\nRelatório Final:\")\n",
    "print(classification_report(df_output['Label'], df_output['Predicted']))\n",
    "print(\"Matriz de Confusão Final:\")\n",
    "print(confusion_matrix(df_output['Label'], df_output['Predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
